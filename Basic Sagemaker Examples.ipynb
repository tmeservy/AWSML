{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SageMaker\n",
    "SageMaker Studio is the hub for Amazon's machine learning services. It is within SageMaker Studio that we can write Python code to import and prepare our data, create and train machine learning models, and automatically deploy endpoints for those models. For those that might be familiar, SageMaker Studio is essentially just a Jupyter Notebooks IDE that has access to all the rest of the AWS services. Since you are already familiar with Google Colab, using Sagemaker shouldn't be too difficult.\n",
    "\n",
    "In this file, you'll learn how to save data from external sources to the local file system that is associated with your SageMaker Studio account and you will also learn how to save and retrieve these files to an S3 bucket.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data and Saving it Locally\n",
    "We can retrieve data from a variety of sources as you've seen in previous classes. It is important to recognize that SageMaker has a local file system of its own. If we wanted to get more techincal, when we created the SageMaker Studio environment, AWS provisioned a filesystem for us and when we launch SageMaker Studio, it mounts that filesystem for us. So, we can simply retrieve and save our data locally (like we've done before).\n",
    "\n",
    "If you would like, you can simply upload a file to your SageMaker file system. However, if you are using AWS, it is likely that you have a lot of data and so rather than saving it here, you might as well upload it directly into S3 where other AWS ML services can also get access.\n",
    "\n",
    "Like you've seen previously, we can also get data directly from a file that is posted on the internet. Panda's read_csv method has the ability to read from a website built in to its method.\n",
    "\n",
    "### Importing a CSV from a URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV from a URL\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('http://www.ishelp.info/data/insurance.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our data is loaded, we usually will take time to analyze and evaluate the dataset. We can use the same methods that we saw in earlier chapters of the book. Once we are done cleaning and transforming the data, we can easily save it to the local file system. To save our data, we can call the to_csv function off of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to local file system\n",
    "data.to_csv('insurance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you run the code block above the file will appear on the local file system in the same directory as your Python notebook. Sometimes it take a few seconds for SageMaker Studio to refresh before it appears.\n",
    "\n",
    "If you double click on the file in the local file system, SageMaker Studio will open your imported data file so that you can explore the contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing JSON from an API\n",
    "\n",
    "Just for fun, let's import some JSON from an API. To keep it simple, this one doesn't require an API key but based on previous classes, you could do a more complicated request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json \n",
    "\n",
    "# '.get' refers to the type of request: GET, POST, or many others but those two are most common\n",
    "response = requests.get(\"https://api.coinlore.com/api/tickers/\") \n",
    "\n",
    "#format the json and load it into an object\n",
    "json_data = json.loads(response.text)\n",
    "\n",
    "#display the data\n",
    "json_data['data']\n",
    "\n",
    "#save to the local file system\n",
    "with open('coins.json', 'w') as f:\n",
    "    f.write(json.dumps(json_data['data']))\n",
    "    f.close\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, we might just save this JSON as to a local file. However, we also might want to convert it into a data frame to make it easier to process in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the coins list in our JSON object to a pandas data frame\n",
    "coins = pd.DataFrame(json_data['data'])\n",
    "\n",
    "# then saving out is super simple\n",
    "coins.to_csv('coins.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the coins data\n",
    "coins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Files to S3\n",
    "\n",
    "Amazon S3 is one of AWS' major services. As you likely know, S3 is an object repository that allows you to store  enormous amounts of data. S3 organizes files or objects in folders called 'buckets.' Each file can be up to 5 terrabytes in size and you can have an unlimited number of files in any S3 bucket. S3 was intentionally built with minimal feature set that focuses on simplicity and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of S3\n",
    "Amazon S3 is intentionally built with a minimal feature set that focuses on simplicity and robustness. Following are some of the advantages of using Amazon S3:\n",
    "\n",
    "* Creating buckets – Create and name a bucket that stores data. Buckets are the fundamental containers in Amazon S3 for data storage.\n",
    "* Storing data – Store an infinite amount of data in a bucket. Upload as many objects as you like into an Amazon S3 bucket. Each object can contain up to 5 TB of data. Each object is stored and retrieved using a unique developer-assigned key.\n",
    "* Downloading data – Download your data or enable others to do so. Download your data anytime you like, or allow others to do the same.\n",
    "* Permissions – Grant or deny access to others who want to upload or download data into your Amazon S3 bucket. Grant upload and download permissions to three types of users. Authentication mechanisms can help keep data secure from unauthorized access.\n",
    "* Standard interfaces – Use standards-based REST and SOAP interfaces designed to work with any internet-development toolkit.\n",
    "\n",
    "[Source: AWS S3 User Guide](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html)\n",
    "\n",
    "### Installing the S3 Python Library\n",
    "We can save files to and retrieve files from S3 using the _s3fs_ Python library. As you may have guessed, this library allows us to interact with S3 like we would any other file system. The following code installs that library on our SageMaker Studio's container so that it is available to all of our scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q 's3fs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More code libraries\n",
    "\n",
    "When you use SageMaker Studio, you launch it from an account that can access a lot of other interesting AWS services without some of the complexities (e.g., authentication/authorization) that you would have to handle if you did it in your own Python Environment. Since you are running within your own AWS account, when you write code within SageMaker Studio you can use existing libraries that provide information about your current session including the default S3 bucket that is assigned to your SageMaker environment. Of course, we could also override that bucket name.\n",
    "\n",
    "However, you can also specify you own S3 bucket if you want to organize it differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries that we will use\n",
    "import s3fs\n",
    "import sagemaker\n",
    "\n",
    "# Get SageMaker session & default S3 bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket() \n",
    "# replace with your own bucket if you have one \n",
    "#bucket = \"PutYourBucketNameHere\" #e.g., sagemaker-us-east-1-427325791960\n",
    "s3 = sagemaker_session.boto_session.resource('s3')\n",
    "\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To upload a file to an S3 bucket, we can use the _upload_file_ method off of the s3 bucket. The following code will upload..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.Bucket(bucket).upload_file(\"coins.csv\",\"projectdata/coins.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are going to be writing a lot of files to S3, it might be helpful to create a function that combines the directory information into an easy to use function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to save files to S3\n",
    "def save_to_s3(filename, s3bucket, s3directory):\n",
    "    key = \"{}/{}\".format(s3directory,filename)\n",
    "    return s3.Bucket(bucket).upload_file(filename,key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use our function to upload the _coins.json_ file and also the _insurance.csv_ file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_s3(\"insurance.csv\",bucket,\"projectdata\")\n",
    "save_to_s3(\"coins.json\",bucket,\"projectdata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving Files from S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main ways to import or ingest your data stored on S3 so that you can use it in building your machine learning models:\n",
    "* Copy the data to your SageMaker Studio and then load it from there\n",
    "* Use pre-built packages that work with S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1 - Copy Data to SageMaker Studio, Then Use it\n",
    "AWS has a Command Line Interface (CLI) that you can use to copy your data from s3 to your SageMaker instance. Then, you can load your data from the local file system. This is fine for up to medium sized data files. More info about the CLI can be found [here](https://docs.aws.amazon.com/cli/latest/reference/s3/cp.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy data to your sagemaker instance using AWS CLI\n",
    "# the first parameter is the bucket name and folder in the bucket, the second parameter is the local directory name\n",
    "!aws s3 cp s3://$bucket/projectdata \"projectdata\" --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2 - Use Pre-built Packages That Work with S3\n",
    "For large dataset or to simply keep all of you data in one place, you can use pre-built packages to directly access your files in S3 without having to copy them to your local file system. The version of _Pandas_ that runs on SageMaker has been adjusted to be able to use an S3 filepath (i.e., 's3://') much like you can use other prefixes (e.g., 'file://', 'https://', 'ftp://') to access files stored either locally or accessible through remote services. More information can be found [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_file = \"s3://{}/{}/{}\".format(bucket, \"projectdata\", \"coins.json\")\n",
    "print(s3_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = pd.read_json(s3_file, orient='records')\n",
    "df_json.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also read other files, like CSV files, directly from their location on S3. You can hardcode the URL if you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example hard coded URL\n",
    "#df_csv = pd.read_csv(\"s3://sagemaker-us-east-1-427325791960/projectdata/insurance.csv\")\n",
    "    \n",
    "#since we are dynamically specifying our bucket, we will format the string and pass in the values    \n",
    "df_csv = pd.read_csv(\"s3://{}/{}/{}\".format(bucket, \"projectdata\", \"insurance.csv\"))\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever possible, I recommend using this second method for referencing your model data. With this approach you aren't constantly shifting data back and forth between environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the S3 File System Directly\n",
    "Sometimes you will want to interact with your files on S3 (e.g., get a list of all files that need to be processed). The _s3fs_ library is a Pythonic file interface to S3. It builds on top of botocore. The top-level class S3FileSystem holds connection information and allows typical file-system style operations like cp, mv, ls, glob, etc., as well as put/get of local files to/from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem()\n",
    "data_s3fs_location = \"s3://{}/{}/\".format(bucket, \"projectdata\")\n",
    "# To List all files in your accessible bucket\n",
    "fs.ls(data_s3fs_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open it directly with s3fs\n",
    "data_s3fs_location = \"s3://{}/{}/{}\".format(bucket, \"projectdata\", \"insurance.csv\") # S3 URL\n",
    "with fs.open(data_s3fs_location) as f:\n",
    "    print(pd.read_csv(f, sep = '\\t', nrows = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An then, you could process each file. In this case, we will simply output a few lines. More likely, you might load each file one by one into a dataframe and process each of the records in all of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
