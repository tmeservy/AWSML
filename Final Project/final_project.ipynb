{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve/Scrape the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAID7MAEAAAAA5p8qahzG1zAHFQYYM7ACWUO3%2FWU%3DrfOMy0JdqDPZ66fgHcl5Dfdfr0OanE3weJlMy2J6eu98P9Khf3'\n",
    "headers = {'Authorization':('Bearer '+ bearer_token)}\n",
    "\n",
    "# In this example, only those tweets with photos/images are stored\n",
    "\n",
    "n = 5 #500                          # The total number of tweets we want\n",
    "max_results = 100                 # The number of tweets to pull per request; must be between 10 and 100\n",
    "next_token = \"\"                   # Must be empty on first iteration\n",
    "search_term = \"covid\"  # To form an advanced query, see here: https://twitter.com/search-advanced?lang=en\n",
    "since_id = \"1504999000000000000\"  # The id of the oldest tweet you want to retrieve\n",
    "\n",
    "# Create the empty DataFrame with the columns you want\n",
    "df = pd.DataFrame(columns=['id', 'created_at', 'retweets', 'likes', 'replies', 'quotes', 'user_followers', 'user_following', 'user_listed', 'user_tweets', 'has_media', 'url', 'lang', 'text'], dtype=object)\n",
    "df.set_index('id', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_retrieved = 0               # To keep track of when to stop\n",
    "\n",
    "# stop when we have n results\n",
    "while total_retrieved < n:\n",
    "\n",
    "  # the first time through the loop, we do not need the next_token parameter\n",
    "  if next_token == \"\":\n",
    "    url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}&since_id={since_id}'\n",
    "  else:\n",
    "    url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}&since_id={since_id}&next_token={next_token}'\n",
    "\n",
    "  # These are the extra parameters we will add to the querystring; we won't store them all though; just want you to see what's possible\n",
    "  url += f'&user.fields=id,public_metrics'\n",
    "  url += f'&tweet.fields=attachments,public_metrics,text,created_at,author_id,lang'\n",
    "  url += f'&expansions=attachments.media_keys,author_id'\n",
    "  url += f'&media.fields=media_key,type,url'\n",
    "\n",
    "  # make the request to the Twitter API Recent Search endpoint\n",
    "  response = requests.request(\"GET\", url, headers=headers)\n",
    "  try:  # Just in case we get an error\n",
    "    json_data = json.loads(response.text)\n",
    "  except:\n",
    "    print(response.text)\n",
    "\n",
    "    \n",
    "  # Error checking; print the results if valid data is not retrieved\n",
    "  if not 'data' in json_data:\n",
    "    json_clean = json.dumps(json_data, indent=2, sort_keys=True)\n",
    "    print(json_clean)\n",
    "    continue\n",
    "\n",
    "\n",
    "  for tweet in json_data['data']:\n",
    "    media_key = \"\"  # Reset to empty each time through the loop so that we can use it for a condition later\n",
    "\n",
    "    # Store the data into variables\n",
    "    tweet_id = tweet['id']\n",
    "    try:\n",
    "      author_id = tweet['author_id']\n",
    "    except:\n",
    "      print(tweet)\n",
    "    created_at = tweet['created_at']\n",
    "    retweet_count = tweet['public_metrics']['retweet_count']\n",
    "    like_count = tweet['public_metrics']['like_count']\n",
    "    reply_count = tweet['public_metrics']['reply_count']\n",
    "    quote_count = tweet['public_metrics']['quote_count']\n",
    "    user_followers = \"\"\n",
    "    user_following = \"\"\n",
    "    user_listed = \"\"\n",
    "    user_tweets = \"\"\n",
    "    has_media = False\n",
    "    image_url = \"\"\n",
    "    lang = tweet['lang']\n",
    "    text = tweet['text']\n",
    "\n",
    "    # Find out if there is media\n",
    "    if 'attachments' in tweet:\n",
    "      if 'media_keys' in tweet['attachments']:\n",
    "        media_key = tweet['attachments']['media_keys'][0]\n",
    "        \n",
    "    # Iterate through all authors until we find the author of this tweet; then store their metrics\n",
    "    for author in json_data['includes']['users']:\n",
    "      if author['id'] == author_id:\n",
    "        user_followers = author['public_metrics']['followers_count']\n",
    "        user_following = author['public_metrics']['following_count']\n",
    "        user_listed = author['public_metrics']['listed_count']\n",
    "        user_tweets = author['public_metrics']['tweet_count']\n",
    "        break\n",
    "\n",
    "    # If there is a media key in this tweet, iterate through tweet['includes']['media'] until we find it\n",
    "    if media_key != \"\":\n",
    "      for media in json_data['includes']['media']:\n",
    "        if media['media_key'] == media_key: # Only if the media_key matches the one we stored\n",
    "          has_media = True\n",
    "          if media['type'] == 'photo':      # Only if it is a photo; ignore videos\n",
    "            image_url = media['url']        # Store the url in a variable\n",
    "            \n",
    "            # Only collect english tweets (to aid the natural language processing) that include a .jpg photo\n",
    "            if (lang == 'en') and (image_url.split('.')[-1] == 'jpg'):\n",
    "              total_retrieved += 1\n",
    "              df.loc[tweet_id] = [created_at, retweet_count, like_count, reply_count, quote_count, user_followers, user_following, user_listed, user_tweets, has_media, image_url, lang, text]\n",
    "            else:\n",
    "              continue\n",
    "            break\n",
    "\n",
    "  # keep track of where to start next time, but quit if there are no more results\n",
    "  try:\n",
    "    #  total_retrieved += json_data['meta']['result_count'] # Use this when you have no other criterion for which tweets to keep\n",
    "    next_token = json_data['meta']['next_token']\n",
    "  except:\n",
    "    break\n",
    "    \n",
    "  print(f'{total_retrieved}, ', end='') # This simply shows something in the output so that we know the loop is running\n",
    "\n",
    "# Parse out the date into potentially useful features\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "df['Weekday'] = df['created_at'].dt.day_name()\n",
    "df['DayOfWeek'] = df['created_at'].dt.dayofweek\n",
    "df['Hour'] = df['created_at'].dt.hour\n",
    "df.to_csv('twitter.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (i.e. Data Understanding Phase)\n",
    "### Begin with univariate analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('twitter.csv')\n",
    "# Convert these numbers to categories\n",
    "df['id'] = df['id'].astype('object')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['created_at'], inplace=True)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  \n",
    "### Continue with bivariate analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap over a correlation table\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "matrix = np.triu(df.corr())\n",
    "sns.heatmap(df.corr(), annot=True, fmt='.2f', \n",
    "            vmin=-1, vmax=1, center=0, cmap= 'coolwarm', \n",
    "            mask=matrix, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(color_codes=True)\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "sns.jointplot(x='likes', y='quotes', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df, x=\"has_media\", y=\"retweets\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pyLDAvis\n",
    "!pip install pyLDAvis.gensim\n",
    "!pip install bokeh\n",
    "!pip install gensim\n",
    "!pip install spacy\n",
    "!pip install logging\n",
    "!pip install wordcloud\n",
    "!pip install warnings\n",
    "!pip install matplotlib\n",
    "!pip install nltk\n",
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install -U seaborn\n",
    "!pip install translators --upgrade\n",
    "#!conda install -c conda-forge pyldavis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate Non-English Tweets to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('twitter.csv')\n",
    "# Convert these numbers to categories\n",
    "df['id'] = df['id'].astype('object')\n",
    "print(df.shape)\n",
    "df.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "  import translators as ts\n",
    "  translated = \"\"\n",
    "\n",
    "  # professional field\n",
    "  try:\n",
    "    translated = ts.alibaba(text, professional_field='general') # (\"general\",\"message\",\"offer\")\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  if translated == \"\":\n",
    "    try:\n",
    "      translated = ts.baidu(text, professional_field='common') # ('common','medicine','electronics','mechanics')\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  # host service\n",
    "  if translated == \"\":\n",
    "    try:\n",
    "      translated = ts.google(text, if_use_cn_host=True)\n",
    "      translated = ts.bing(text, if_use_cn_host=False)\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  return translated\n",
    "\n",
    "for i, row in enumerate(df.itertuples()):\n",
    "  if row[13] != 'en':\n",
    "#     df.loc[row[0]] = translate(df.loc[row[0]][13])\n",
    "    print(row[13], df.loc[row[0]][13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('twitter.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Stop Words List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings, en_core_web_sm\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 'co', 'https', 'http', 'twitter', 'amp', 'covid', 'gofundme']) # After reviewing the LDA, return to add words that you want to eliminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Clean\n",
    "Remove line breaks, single quotes, email addresses.\n",
    "Use Gensim's simple_preprocess to hash/tokenize each string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\\\S*@\\\\S*\\\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  \n",
    "\n",
    "# Convert each tweet to a list of cleaned words and add to a master list\n",
    "data = df.text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "for tweet in data_words[:5]: # print the first :n tweet word lists\n",
    "  print(tweet)\n",
    "\n",
    "df['words'] = data_words\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = ''\n",
    "for i in range(len(data_words)):\n",
    "    for j in range(len(data_words[i])):\n",
    "        length += data_words[i][j]\n",
    "print(f'Corpus size: {str(len(length))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Bigrams, Trigrams, and Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words, add bigrams and trigrams, performed lemmatization/stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and perform Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    \n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])    # Load spacy, but we don't need the parser or NER (named entity extraction) modules\n",
    "    \n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        \n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(data_words)\n",
    "for tweet in data_ready[:5]:\n",
    "  print(tweet)\n",
    "\n",
    "df['words'] = data_ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LDA Topic Model: Tweet Text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an LDA \n",
    "Build Latent Dirichlet Allocation model for detecting the top n topics in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(data_ready=None, id2word=None, corpus=None, start=2, iterations=10, every=2):\n",
    "  coherence_list = []\n",
    "    \n",
    "  print(f'Topics\\tPerplexity\\tCoherence')\n",
    "  for topics in range(start, (start + iterations) * every, every):\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=topics, random_state=100,\n",
    "                                                update_every=1, chunksize=20, passes=20, alpha='symmetric',\n",
    "                                                iterations=500,per_word_topics=True)\n",
    "\n",
    "    # Compute LDA metrics\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_ready, dictionary=id2word, coherence='c_v')\n",
    "    print(f'{topics}\\t{round(lda_model.log_perplexity(corpus), 4)}\\t\\t{round(coherence_model_lda.get_coherence(), 4)}')\n",
    "    coherence_list.append(coherence_model_lda.get_coherence())\n",
    "\n",
    "  # Determine the numer of topics for the LDA with the highest coherence score\n",
    "  best_topics = (coherence_list.index(max(coherence_list)) + start) * every\n",
    "    \n",
    "  lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=best_topics, random_state=100,\n",
    "                                              update_every=1, chunksize=20, passes=20, alpha='symmetric',\n",
    "                                              iterations=500,per_word_topics=True)\n",
    "\n",
    "  ldatopics = lda_model.show_topics(formatted=False)\n",
    "  pprint(lda_model.print_topics())\n",
    "  return lda_model\n",
    "\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "lda_model = lda(data_ready, id2word, corpus, start=2, iterations=9, every=1)\n",
    "num_topics = len(lda_model.get_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dominant Topic\n",
    "What is the Dominant topic and its % contribution in each tweet?\n",
    "\n",
    "In LDA models, each document is composed of multiple topics. But, typically only one of the topics is dominant. The below code extracts this dominant topic for each sentence and shows the weight of the topic and the keywords in a nicely formatted output.\n",
    "\n",
    "This way, you will know which document belongs predominantly to which topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_topic_scores(ldamodel=None, corpus=None, texts=data, df=df):\n",
    "  # Create the new, zeroed columns to store the topic scores, dominant topic, and dominant topic score\n",
    "  df['Dominant_topic'] = 0\n",
    "  df['Dominant_score'] = 0.0\n",
    "  num_topics = len(ldamodel.get_topics())\n",
    "  for col in range(num_topics):\n",
    "    df[f'topic_{col + 1}'] = 0.0\n",
    "    \n",
    "  # Store the topic score and dominant topic\n",
    "  for i, words in enumerate(texts):\n",
    "    doc = ldamodel[id2word.doc2bow(words)] # generate a corpus for this document set of workds\n",
    "        \n",
    "    for j, score in enumerate(doc[0]):\n",
    "      df.iat[i, (len(df.columns) - ((num_topics) - score[0]))] = score[1]\n",
    "        \n",
    "    topic_score_list = [x[1] for x in doc[0]]\n",
    "    dominant_topic = topic_score_list.index(max(topic_score_list))\n",
    "    df.at[i, 'Dominant_topic'] = dominant_topic + 1\n",
    "    df.at[i, 'Dominant_score'] = topic_score_list[dominant_topic]\n",
    "    \n",
    "  return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = store_topic_scores(lda_model, corpus, data_ready)\n",
    "df.to_csv(f'twitter_with_LDA.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualize the LDA Topics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution\n",
    "How many words are in each tweet? When working with a large number of tweets, you want to know how big the tweet are as a whole and by topic. Let’s plot the tweet word counts distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lens = [len(d) for d in df.words]\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "plt.figure(figsize=(18,7), dpi=100)\n",
    "sns.distplot(doc_lens)\n",
    "plt.text(-7, .048, \"Mean   : \" + str(round(np.mean(doc_lens), 2)))\n",
    "plt.text(-7, .046, \"Median : \" + str(round(np.median(doc_lens), 2)))\n",
    "plt.text(-7, .044, \"Stdev   : \" + str(round(np.std(doc_lens), 2)))\n",
    "plt.text(-7, .042, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01), 2)))\n",
    "plt.text(-7, .040, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99), 2)))\n",
    "\n",
    "plt.gca().set(ylabel='Number of Tweets', xlabel='Tweet Word Count')\n",
    "plt.tick_params(size=16)\n",
    "plt.xticks(np.linspace(0,27,28))\n",
    "plt.title('Distribution of Tweet Word Counts', fontdict=dict(size=22))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counts by Dominant Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import math\n",
    "cols = [color for name, color in mcolors.XKCD_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS' or 'mcolors.CSS4_COLORS'\n",
    "\n",
    "fig, axes = plt.subplots(math.ceil(num_topics**(1/2)), math.ceil(num_topics**(1/2)), figsize=(16,14), dpi=160, sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):    \n",
    "    df_sub = df.loc[df.Dominant_topic == (i + 1), :]\n",
    "    doc_lens = [len(d) for d in df_sub.words]\n",
    "    ax.hist(doc_lens, color=cols[i])\n",
    "    ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n",
    "    sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx(), bw=1.5)\n",
    "    ax.set(xlim=(0, 28), xlabel='')\n",
    "    ax.set_ylabel('Number of Documents', color=cols[i])\n",
    "    ax.set_title('Topic: '+str(i + 1), fontdict=dict(size=16, color=cols[i]))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.90)\n",
    "plt.xticks(np.linspace(0,27,28))\n",
    "fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clouds of Top N Keywords\n",
    "Update the max_words variable below to include more or less words per cloud. The coloring of the topics is used in subsequent vizs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.XKCD_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS', fewer colors: 'mcolors.TABLEAU_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=20,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(math.ceil(num_topics**(1/2)), math.ceil(num_topics**(1/2)), figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    try:\n",
    "      topic_words = dict(topics[i][1])\n",
    "      cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "      plt.gca().imshow(cloud)\n",
    "      plt.gca().set_title('Topic ' + str(i+1), fontdict=dict(size=16))\n",
    "      plt.gca().axis('off')\n",
    "    except:\n",
    "      continue\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Keywords Counts\n",
    "When it comes to the keywords in the topics, the importance (weights) of the keywords matters. Along with that, how frequently the words have appeared in the tweets is also interesting to see.\n",
    "\n",
    "We will plot the word counts and the weights of each keyword in the same chart.\n",
    "\n",
    "Look for words that occur in multiple topics and the ones whose relative frequency is more than the weight. Often such words turn out to be less important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of word counts for each topic\n",
    "from collections import Counter\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data_ready for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i + 1, weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(math.ceil(num_topics**(1/2)), math.ceil(num_topics**(1/2)), figsize=(20,20), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.XKCD_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i+1, :], color=cols[i+1], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i+1, :], color=cols[i+1], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i+1])\n",
    "    # ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
    "    ax.set_title('Topic: ' + str(i + 1), color=cols[i+1], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i+1, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    try:\n",
    "      ax.legend(loc='upper center'); ax_twin.legend(loc='upper right')\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Chart Colored \n",
    "Each word in a tweet is representative of one of the 4 topics. You can color each word in a given tweet by the topic id it is attributed to.\n",
    "The color of the enclosing rectangle is the topic assigned to the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Coloring of N Tweets\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def sentences_chart(lda_model=lda_model, corpus=corpus, start = 0, end = 13):\n",
    "    corp = corpus[start:end]\n",
    "    mycolors = [color for name, color in mcolors.XKCD_COLORS.items()]\n",
    "\n",
    "    fig, axes = plt.subplots(end-start, 1, figsize=(20, (end-start)*0.95), dpi=160)       \n",
    "    axes[0].axis('off')\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i > 0:\n",
    "            corp_cur = corp[i-1] \n",
    "            topic_percs, wordid_topics, wordid_phivalues = lda_model[corp_cur]\n",
    "            word_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics]    \n",
    "            ax.text(0.01, 0.5, \"Doc \" + str(i) + \": \", verticalalignment='center',\n",
    "                    fontsize=16, color='black', transform=ax.transAxes, fontweight=700)\n",
    "\n",
    "            # Draw Rectange\n",
    "            topic_percs_sorted = sorted(topic_percs, key=lambda x: (x[1]), reverse=True)\n",
    "            ax.add_patch(Rectangle((0.0, 0.05), 0.99, 0.90, fill=None, alpha=1, \n",
    "                                   color=mycolors[topic_percs_sorted[0][0]], linewidth=2))\n",
    "\n",
    "            word_pos = 0.06\n",
    "            for j, (word, topics) in enumerate(word_dominanttopic):\n",
    "                if j < 14:\n",
    "                    ax.text(word_pos, 0.5, word,\n",
    "                            horizontalalignment='left',\n",
    "                            verticalalignment='center',\n",
    "                            fontsize=16, color=mycolors[topics],\n",
    "                            transform=ax.transAxes, fontweight=700)\n",
    "                    word_pos += .009 * len(word)  # to move the word for the next iter\n",
    "                    ax.axis('off')\n",
    "            ax.text(word_pos, 0.5, '. . .',\n",
    "                    horizontalalignment='left',\n",
    "                    verticalalignment='center',\n",
    "                    fontsize=16, color='black',\n",
    "                    transform=ax.transAxes)       \n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.suptitle('Sentence Topic Coloring for Tweets: ' + str(start + 1) + ' to ' + str(end-1), fontsize=22, y=0.95, fontweight=700)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "sentences_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Topics\n",
    "What are the most discussed topics in the tweets? We can compute the total number of tweets attributed to each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of the most dominant topics and then the three top keywords in each of those topics\n",
    "def topics_per_document(model, corpus, start=0, end=1):\n",
    "    corpus_sel = corpus[start:end]\n",
    "    dominant_topics = []\n",
    "    topic_percentages = []\n",
    "    for i, corp in enumerate(corpus_sel):\n",
    "        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n",
    "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
    "        dominant_topics.append((i, dominant_topic))\n",
    "        topic_percentages.append(topic_percs)\n",
    "    return(dominant_topics, topic_percentages)\n",
    "\n",
    "dominant_topics, topic_percentages = topics_per_document(model=lda_model, corpus=corpus, end=-1)            \n",
    "\n",
    "# Distribution of Dominant Topics in Each Document\n",
    "df = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\n",
    "dominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\n",
    "df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n",
    "\n",
    "# Total Topic Distribution by actual weight\n",
    "topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n",
    "df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n",
    "\n",
    "# Top 3 Keywords for each Topic\n",
    "topic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n",
    "                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n",
    "\n",
    "df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
    "df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
    "df_top3words.reset_index(level=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two Plots:\n",
    "#    Num tweets per topic by assigning the document to the topic that has the most weight in that document.\n",
    "#    Num tweets per topic by summing up the actual weight contribution of each topic to respective documents.\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 4), dpi=120, sharey=True)\n",
    "\n",
    "# Topic Distribution by Tweet Topics\n",
    "ax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='firebrick')\n",
    "ax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\n",
    "tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x+1)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\n",
    "ax1.xaxis.set_major_formatter(tick_formatter)\n",
    "ax1.set_title('Number of Tweets by Tweet Topic', fontdict=dict(size=10))\n",
    "ax1.set_ylabel('Number of Tweets')\n",
    "# ax1.set_ylim(0, 1000)\n",
    "\n",
    "# Topic Distribution by Topic Weights\n",
    "ax2.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='steelblue')\n",
    "ax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\n",
    "ax2.xaxis.set_major_formatter(tick_formatter)\n",
    "ax2.set_title('Number of Tweets by Topic Weightage', fontdict=dict(size=10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE Clustering Chart\n",
    "Compute the total number of tweets attributed to each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic weights and dominant topics\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "for i, row_list in enumerate(lda_model[corpus]):\n",
    "    topic_weights.append([w for i, w in row_list[0]])\n",
    "\n",
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "\n",
    "# Keep the well separated points (optional)\n",
    "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "# Plot the Topic Clusters using Bokeh\n",
    "output_notebook()\n",
    "n_topics = 4\n",
    "mycolors = np.array([color for name, color in mcolors.XKCD_COLORS.items()])\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
    "              plot_width=900, plot_height=700)\n",
    "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyLDAVis\n",
    "Finally, pyLDAVis is the most commonly used and a nice way to visualise the information contained in a topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some reason, pyLDAVis doesn't seem to work in this AWS kernel. It appears there is a conflict between some of the conda libraries.\n",
    "# import pyLDAvis.gensim\n",
    "# pyLDAvis.enable_notebook()\n",
    "# viz = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "# viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset with LDA Topic Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('twitter_with_LDA.csv')\n",
    "# Convert these numbers to categories\n",
    "df['id'] = df['id'].astype('object')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Images\n",
    "\n",
    "First, we will download all of the images we want to process and store them in S3. Let's first define a method that allows us to download an image from a URL and save it locally. The second block of code simply iterates over our data set and passes the URL and filename to the download_image function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, name):\n",
    "  import requests\n",
    "  file_type = url.split('.')[-1]\n",
    "  img_data = requests.get(url).content\n",
    "  try:\n",
    "    with open(f'images/{name}.{file_type}', 'wb') as handler:\n",
    "      handler.write(img_data)\n",
    "  except:\n",
    "    with open(f'images/{name}.{file_type}', 'wb') as handler:\n",
    "      handler.write(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(df.itertuples()):\n",
    "  if not pd.isnull(row.url):\n",
    "    download_image(row.url, row.id)\n",
    "    print(i, row.url, row.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save them to S3 Bucket\n",
    "Now that we have all of the files stored locally, we need to upload them to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "bucket = \"tmeservy-mldata\"    # replace this with your bucket name\n",
    "prefix = \"photos/twitter\" # replace this with the path to your images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload the files to the S3 bucket\n",
    "images = glob.glob(\"images/*.jpg\")\n",
    "for filename in images:\n",
    "  boto3.Session().resource('s3').Bucket(bucket).upload_file(filename,f'{prefix}/{os.path.basename(filename)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use AWS Rekognition to Scrape Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('twitter_with_LDA.csv')\n",
    "# Convert these numbers to categories\n",
    "df['id'] = df['id'].astype('object')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, you may want to limit the number of rows that you are processing until you get your code working just right. You can use the following code to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit number of rows for testing-if you don't want to process everything\n",
    "#df=df.head(40)\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the rekognition client\n",
    "client = boto3.client('rekognition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the s3 client\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(bucket)\n",
    "#files = my_bucket.objects.filter(Prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous classes when we processed images with Rekognition we retrieved a list of all files in S3 and then passed each file from the list to Rekognition. Here, instead, we will iterate over our dataframe and construct the name of the file we want it to process from the TweetID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFaces = pd.DataFrame([])\n",
    "i=0\n",
    "\n",
    "for row in df.itertuples():\n",
    "    file = f\"{row.id}\"\n",
    "    extension = f\"{row.url.split('.')[-1]}\"\n",
    "    print(f\"Processing TweetID - {file}.{extension}\")    \n",
    "    if not pd.isnull(file):\n",
    "        #print(i, row[1], file, f\"{prefix}/{row[1]}.{file.split('.')[-1]}\")\n",
    "    \n",
    "    \n",
    "        if extension == 'jpg':\n",
    "            # call rekognition for this next file\n",
    "            response = client.detect_faces(\n",
    "                Image={\n",
    "                    'S3Object': {\n",
    "                        'Bucket': bucket,\n",
    "                        'Name': f\"{prefix}/{file}.{extension}\"\n",
    "                    }\n",
    "                },\n",
    "                Attributes=[\n",
    "                    'ALL',\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # now add all of the facial features for every person found in the photo\n",
    "            for fd in response[\"FaceDetails\"]:    \n",
    "                i=i+1\n",
    "                dfFaces.loc[i,'TweetID'] = file\n",
    "                dfFaces.loc[i,'PersonID'] = i\n",
    "                dfFaces.loc[i,'AgeRange-Low'] = fd[\"AgeRange\"][\"Low\"]\n",
    "                dfFaces.loc[i,'AgeRange-High'] = fd[\"AgeRange\"][\"High\"]\n",
    "                dfFaces.loc[i,'Smile'] = fd[\"Smile\"][\"Value\"]\n",
    "                dfFaces.loc[i,'Gender'] = fd[\"Gender\"][\"Value\"]\n",
    "                dfFaces.loc[i,'Emotion'] = fd[\"Emotions\"][0][\"Type\"] #get dominant emotion\n",
    "                dfFaces.loc[i,'Emotion-Confidence'] = fd[\"Emotions\"][0][\"Confidence\"] #get dominant emotion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFaces.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we extracted information from people that were detected in the image. Of course, we could have used other Rekgonition methods to extract text or identify different types of objects that were identified. We now need to summarize this data into a single row for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFaceSummary = pd.DataFrame([])\n",
    "\n",
    "#summarize stats per file\n",
    "#aggregate functions include min, max, mean, count, and more.\n",
    "dfFaceSummary['Count-People'] = dfFaces.groupby('TweetID')['PersonID'].count()\n",
    "dfFaceSummary['Avg-AgeRange-Low'] = dfFaces.groupby('TweetID')['AgeRange-Low'].min()\n",
    "dfFaceSummary['Avg-AgeRange-High'] = dfFaces.groupby('TweetID')['AgeRange-High'].max()\n",
    "dfFaceSummary['Count-Smile'] = dfFaces[dfFaces['Smile']==True].groupby('TweetID')['Smile'].count()\n",
    "dfFaceSummary['Count-Smile'] = dfFaceSummary['Count-Smile'].fillna(0)\n",
    "dfFaceSummary['Percent-Smile'] = dfFaceSummary['Count-Smile']/df2['Count-People']\n",
    "dfFaceSummary['Percent-Smile'] = dfFaceSummary['Percent-Smile'].fillna(0)\n",
    "\n",
    "dfFaceSummary = dfFaceSummary.merge(dfFaces.groupby('TweetID')['Emotion'].value_counts().unstack().fillna(0), on='TweetID')\n",
    "dfFaceSummary = dfFaceSummary.merge(dfFaces.groupby('TweetID')['Gender'].value_counts().unstack().fillna(0), on='TweetID')\n",
    "\n",
    "#print out \n",
    "dfFaceSummary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have summary information, we need to merge it back to our data frame and save it to a CSV file so that we can use it during modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the tweet id is an object, so we need to convert it to the same data type as our id\n",
    "dfFaceSummary.index = dfFaceSummary.index.astype(int)\n",
    "\n",
    "#Now merge the frames together\n",
    "df = df.merge(dfFaceSummary, left_on='id', right_on='TweetID')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now save this out to a new csv\n",
    "df.to_csv('twitter_with_LDA_and_image_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor # Import Decision Tree Regression algorithm\n",
    "from sklearn.ensemble import GradientBoostingRegressor # Import XGBoost algorithm \n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "# for a completelist of available algorithms: https://scikit-learn.org/stable/supervised_learning.html\n",
    "# Which one should I use?: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "\n",
    "df = pd.read_csv('twitter_with_LDA_and_image_data.csv')\n",
    "# Convert these numbers to categories\n",
    "df['id'] = df['id'].astype('object')\n",
    "print(df.shape)\n",
    "\n",
    "# Determine what you want to predict:\n",
    "label = 'likes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the columns so that we can identify which ones we want to drop (e.g. unique identifiers, original text before processing, image file names, dates)\n",
    "for col in df.columns:\n",
    "  print(f'\\'{col}\\', ', end=\"\")\n",
    "\n",
    "drop_list = ['id', 'created_at', 'url', 'text', 'words', 'File', 'PersonID', 'topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate anything remaining in the drop list without throwing an error in case we removed it earlier\n",
    "for col in df.columns:\n",
    "  if col in drop_list:\n",
    "    try:\n",
    "      df.drop(columns=[col], inplace=True)\n",
    "    except:\n",
    "      continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)       # Remove any rows with null values\n",
    "print(f'Records: {len(df)}')  # Count and print the number of rows remaining\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Dominant_topic'] = df['Dominant_topic'].astype('object') # Topics are categorical so this needs to be modified since it's a number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy codes for all features and not the label\n",
    "for col in df.columns:\n",
    "  if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "    df = pd.get_dummies(df, columns=[col], prefix=col)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate columns with only one unique value:\n",
    "for col in df.columns:\n",
    "  if (df[col].nunique() < 2):\n",
    "    try:\n",
    "      df.drop(columns=[col], inplace=True)\n",
    "    except:\n",
    "      continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset in features and target variable\n",
    "\n",
    "y = df[label] # Label\n",
    "X = df.drop(columns=[label]) # Features\n",
    "X = X.select_dtypes(np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Linear Regression is only for model interpretation purposes\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Run the multiple linear regression model\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# View results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree regressor object\n",
    "clf = DecisionTreeRegressor()\n",
    "\n",
    "# Train Decision Tree regressor\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,})\n",
    "output_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scikit-learn metrics module. See complete list of Classification metrics here: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "from sklearn import metrics\n",
    "    \n",
    "print(f'R squared:\\t{metrics.r2_score(y_test, y_pred)}')\n",
    "print(f'MAE:\\t\\t{metrics.mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'RMSE:\\t\\t{metrics.mean_squared_error(y_test, y_pred)**(1/2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XGBoost regressor object\n",
    "clr = GradientBoostingRegressor()\n",
    "\n",
    "# Train Decision Tree regression\n",
    "clr = clf.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels for test dataset\n",
    "y_pred = clr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,})\n",
    "output_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "    \n",
    "print(f'R squared:\\t{metrics.r2_score(y_test, y_pred)}')\n",
    "print(f'MAE:\\t\\t{metrics.mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'RMSE:\\t\\t{metrics.mean_squared_error(y_test, y_pred)**(1/2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model with the highest fit metric\n",
    "pickle.dump(clr, open('stored_model.sav', 'wb'))  # OPTION 1: pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...some time later\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# OPTION 1: Using pickle\n",
    "# load the model from 'stored_model.sav'\n",
    "loaded_model = pickle.load(open('stored_model.sav', 'rb'))\n",
    "\n",
    "# for a single prediction, enter a row of data and reshape into numpy array\n",
    "case = X_test.iloc[0]\n",
    "print(f'Single prediction:\\t{loaded_model.predict(np.array(case).reshape(1, -1))[0]}\\n\\n{case}\\n')\n",
    "\n",
    "# for a batch prediction, enter a Pandas DataFrame or a Numpy array of arrays\n",
    "predictions = loaded_model.predict(X_test) \n",
    "batch_results = pd.DataFrame({'Actual':y_test, 'Predicted':predictions, 'Diff':(predictions - y_test)})\n",
    "print(f'MAE:\\t{batch_results.Diff.abs().mean()}\\n')\n",
    "batch_results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
