{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve/Scrape the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAGQvMQEAAAAAJ3DsIoP%2FZadSuqr0iajMvuOFyZU%3DjevRvOrRSVMeoTctJGqK5dW2BBdbbRLKE3aOTfbWIhMIhvPViC'\n",
    "headers = {'Authorization':('Bearer '+ bearer_token)}\n",
    "\n",
    "# In this example, only those tweets with photos/images are stored\n",
    "\n",
    "n = 5000                          # The total number of tweets we want\n",
    "max_results = 100                 # The number of tweets to pull per request; must be between 10 and 100\n",
    "total_retrieved = 0               # To keep track of when to stop\n",
    "next_token = \"\"                   # Must be empty on first iteration\n",
    "search_term = \"covid\"  # To form an advanced query, see here: https://twitter.com/search-advanced?lang=en\n",
    "since_id = \"1373000000000000000\"  # The id of the oldest tweet you want to retrieve\n",
    "\n",
    "# Create the empty DataFrame with the columns you want\n",
    "df = pd.DataFrame(columns=['id', 'created_at', 'retweets', 'likes', 'replies', 'quotes', 'user_followers', 'user_following', 'user_listed', 'user_tweets', 'has_media', 'url', 'lang', 'text'])\n",
    "df.set_index('id', inplace=True)\n",
    "\n",
    "# stop when we have n results\n",
    "while total_retrieved < n:\n",
    "\n",
    "  # the first time through the loop, we do not need the next_token parameter\n",
    "  if next_token == \"\":\n",
    "    url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}&since_id={since_id}'\n",
    "  else:\n",
    "    url = f'https://api.twitter.com/2/tweets/search/recent?query={search_term}&max_results={max_results}&since_id={since_id}&next_token={next_token}'\n",
    "\n",
    "  # These are the extra parameters we will add to the querystring; we won't store them all though; just want you to see what's possible\n",
    "  url += f'&user.fields=id,public_metrics'\n",
    "  url += f'&tweet.fields=attachments,public_metrics,text,created_at,author_id,lang'\n",
    "  url += f'&expansions=attachments.media_keys,author_id'\n",
    "  url += f'&media.fields=media_key,type,url'\n",
    "\n",
    "  # make the request to the Twitter API Recent Search endpoint\n",
    "  response = requests.request(\"GET\", url, headers=headers)\n",
    "  try:  # Just in case we get an error\n",
    "    json_data = json.loads(response.text)\n",
    "  except:\n",
    "    print(response.text)\n",
    "  \n",
    "  # Error checking; print the results if valid data is not retrieved\n",
    "  if not 'data' in json_data:\n",
    "    json_clean = json.dumps(json_data, indent=2, sort_keys=True)\n",
    "    print(json_clean)\n",
    "    continue\n",
    "\n",
    "\n",
    "  for tweet in json_data['data']:\n",
    "    media_key = \"\"  # Reset to empty each time through the loop so that we can use it for a condition later\n",
    "\n",
    "    # Store the data into variables\n",
    "    tweet_id = tweet['id']\n",
    "    try:\n",
    "      author_id = tweet['author_id']\n",
    "    except:\n",
    "      print(tweet)\n",
    "    created_at = tweet['created_at']\n",
    "    retweet_count = tweet['public_metrics']['retweet_count']\n",
    "    like_count = tweet['public_metrics']['like_count']\n",
    "    reply_count = tweet['public_metrics']['reply_count']\n",
    "    quote_count = tweet['public_metrics']['quote_count']\n",
    "    user_followers = \"\"\n",
    "    user_following = \"\"\n",
    "    user_listed = \"\"\n",
    "    user_tweets = \"\"\n",
    "    has_media = False\n",
    "    image_url = \"\"\n",
    "    lang = tweet['lang']\n",
    "    text = tweet['text']\n",
    "\n",
    "    # Find out if there is media\n",
    "    if 'attachments' in tweet:\n",
    "      if 'media_keys' in tweet['attachments']:\n",
    "        media_key = tweet['attachments']['media_keys'][0]\n",
    "        \n",
    "    # Iterate through all authors until we find the author of this tweet; then store their metrics\n",
    "    for author in json_data['includes']['users']:\n",
    "      if author['id'] == author_id:\n",
    "        user_followers = author['public_metrics']['followers_count']\n",
    "        user_following = author['public_metrics']['following_count']\n",
    "        user_listed = author['public_metrics']['listed_count']\n",
    "        user_tweets = author['public_metrics']['tweet_count']\n",
    "        break\n",
    "\n",
    "    # If there is a media key in this tweet, iterate through tweet['includes']['media'] until we find it\n",
    "    if media_key != \"\":\n",
    "      for media in json_data['includes']['media']:\n",
    "        if media['media_key'] == media_key: # Only if the media_key matches the one we stored\n",
    "          has_media = True\n",
    "          if media['type'] == 'photo':      # Only if it is a photo; ignore videos\n",
    "            image_url = media['url']        # Store the url in a variable\n",
    "            \n",
    "            # Only collect english tweets (to aid the natural language processing) that include a .jpg photo\n",
    "            if (lang == 'en') and (image_url.split('.')[-1] == 'jpg'):\n",
    "              total_retrieved += 1\n",
    "              df.loc[tweet_id] = [created_at, retweet_count, like_count, reply_count, quote_count, user_followers, user_following, user_listed, user_tweets, has_media, image_url, lang, text]\n",
    "            else:\n",
    "              continue\n",
    "            break\n",
    "\n",
    "  # keep track of where to start next time, but quit if there are no more results\n",
    "  try:\n",
    "    #  total_retrieved += json_data['meta']['result_count'] # Use this when you have no other criterion for which tweets to keep\n",
    "    next_token = json_data['meta']['next_token']\n",
    "  except:\n",
    "    break\n",
    "    \n",
    "  print(f'{total_retrieved}, ', end='') # This simply shows something in the output so that we know the loop is running\n",
    "\n",
    "# Parse out the date into potentially useful features\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "df['Weekday'] = df['created_at'].dt.day_name()\n",
    "df['DayOfWeek'] = df['created_at'].dt.dayofweek\n",
    "df['Hour'] = df['created_at'].dt.hour\n",
    "df.to_csv('twitter.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (i.e. Data Understanding Phase)\n",
    "### Begin with univariate analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('twitter.csv')\n",
    "# Convert these numbers to categories\n",
    "df['id'] = df['id'].astype('object')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['created_at'], inplace=True)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  \n",
    "### Continue with bivariate analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap over a correlation table\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "matrix = np.triu(df.corr())\n",
    "sns.heatmap(df.corr(), annot=True, fmt='.2f', \n",
    "            vmin=-1, vmax=1, center=0, cmap= 'coolwarm', \n",
    "            mask=matrix, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(color_codes=True)\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "sns.jointplot(x='likes', y='quotes', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df, x=\"has_media\", y=\"retweets\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pyLDAvis\n",
    "!pip install pyLDAvis.gensim\n",
    "!pip install bokeh\n",
    "!pip install gensim\n",
    "!pip install spacy\n",
    "!pip install logging\n",
    "!pip install wordcloud\n",
    "!pip install warnings\n",
    "!pip install matplotlib\n",
    "!pip install nltk\n",
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install -U seaborn\n",
    "!pip install translators --upgrade\n",
    "#!conda install -c conda-forge pyldavis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate Non-English Tweets to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('twitter.csv')\n",
    "# Convert these numbers to categories\n",
    "df['id'] = df['id'].astype('object')\n",
    "print(df.shape)\n",
    "df.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "  import translators as ts\n",
    "  translated = \"\"\n",
    "\n",
    "  # professional field\n",
    "  try:\n",
    "    translated = ts.alibaba(text, professional_field='general') # (\"general\",\"message\",\"offer\")\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  if translated == \"\":\n",
    "    try:\n",
    "      translated = ts.baidu(text, professional_field='common') # ('common','medicine','electronics','mechanics')\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  # host service\n",
    "  if translated == \"\":\n",
    "    try:\n",
    "      translated = ts.google(text, if_use_cn_host=True)\n",
    "      translated = ts.bing(text, if_use_cn_host=False)\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  return translated\n",
    "\n",
    "for i, row in enumerate(df.itertuples()):\n",
    "  if row[13] != 'en':\n",
    "#     df.loc[row[0]] = translate(df.loc[row[0]][13])\n",
    "    print(row[13], df.loc[row[0]][13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('twitter.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Stop Words List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings, en_core_web_sm\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 'co', 'https', 'http', 'twitter', 'amp', 'covid', 'gofundme']) # After reviewing the LDA, return to add words that you want to eliminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Clean\n",
    "Remove line breaks, single quotes, email addresses.\n",
    "Use Gensim's simple_preprocess to hash/tokenize each string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\\\S*@\\\\S*\\\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  \n",
    "\n",
    "# Convert each tweet to a list of cleaned words and add to a master list\n",
    "data = df.text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "for tweet in data_words[:5]: # print the first :n tweet word lists\n",
    "  print(tweet)\n",
    "\n",
    "df['words'] = data_words\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = ''\n",
    "for i in range(len(data_words)):\n",
    "    for j in range(len(data_words[i])):\n",
    "        length += data_words[i][j]\n",
    "print(f'Corpus size: {str(len(length))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Bigrams, Trigrams, and Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words, add bigrams and trigrams, performed lemmatization/stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and perform Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    \n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])    # Load spacy, but we don't need the parser or NER (named entity extraction) modules\n",
    "    \n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        \n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(data_words)\n",
    "for tweet in data_ready[:5]:\n",
    "  print(tweet)\n",
    "\n",
    "df['words'] = data_ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LDA Topic Model: Tweet Text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an LDA \n",
    "Build Latent Dirichlet Allocation model for detecting the top n topics in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(data_ready=data_ready, start=2, iterations=10, every=2):\n",
    "  # Create Dictionary\n",
    "  id2word = corpora.Dictionary(data_ready)\n",
    "\n",
    "  # Create Corpus: Term Document Frequency\n",
    "  corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "\n",
    "  coherence_list = []\n",
    "    \n",
    "  print(f'Topics\\tPerplexity\\tCoherence')\n",
    "  for topics in range(start, (start + iterations) * every, every):\n",
    "    # Build LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=topics, random_state=100,\n",
    "                                                update_every=1, chunksize=20, passes=20, alpha='symmetric',\n",
    "                                                iterations=500,per_word_topics=True)\n",
    "\n",
    "    # Compute LDA metrics\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_ready, dictionary=id2word, coherence='c_v')\n",
    "    print(f'{topics}\\t{round(lda_model.log_perplexity(corpus), 4)}\\t\\t{round(coherence_model_lda.get_coherence(), 4)}')\n",
    "    coherence_list.append(coherence_model_lda.get_coherence())\n",
    "\n",
    "  # Determine the numer of topics for the LDA with the highest coherence score\n",
    "  best_topics = (coherence_list.index(max(coherence_list)) + start) * every\n",
    "    \n",
    "  lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=best_topics, random_state=100,\n",
    "                                              update_every=1, chunksize=20, passes=20, alpha='symmetric',\n",
    "                                              iterations=500,per_word_topics=True)\n",
    "\n",
    "  ldatopics = lda_model.show_topics(formatted=False)\n",
    "  pprint(lda_model.print_topics())\n",
    "  return lda_model\n",
    "print(len(data_ready))\n",
    "lda_model = lda(data_ready, start=2, iterations=9, every=1)\n",
    "#print(len(lda_model[corpus]))\n",
    "num_topics = len(lda_model.get_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dominant Topic\n",
    "What is the Dominant topic and its % contribution in each tweet?\n",
    "\n",
    "In LDA models, each document is composed of multiple topics. But, typically only one of the topics is dominant. The below code extracts this dominant topic for each sentence and shows the weight of the topic and the keywords in a nicely formatted output.\n",
    "\n",
    "This way, you will know which document belongs predominantly to which topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_topic_scores(ldamodel=None, corpus=corpus, texts=data, df=df):\n",
    "  \n",
    "  # Create the new, zeroed columns to store the topic scores, dominant topic, and dominant topic score\n",
    "  df['Dominant_topic'] = 0\n",
    "  df['Dominant_score'] = 0.0\n",
    "  num_topics = len(ldamodel.get_topics())\n",
    "  for col in range(num_topics):\n",
    "    df[f'topic_{col + 1}'] = 0.0\n",
    "    \n",
    "  # Store the topic score and dominant topic\n",
    "  for i, words in enumerate(texts):\n",
    "    doc = ldamodel[id2word.doc2bow(words)] # generate a corpus for this document set of workds\n",
    "        \n",
    "    for j, score in enumerate(doc[0]):\n",
    "      df.iat[i, (len(df.columns) - ((num_topics) - score[0]))] = score[1]\n",
    "        \n",
    "    topic_score_list = [x[1] for x in doc[0]]\n",
    "    dominant_topic = topic_score_list.index(max(topic_score_list))\n",
    "    df.at[i, 'Dominant_topic'] = dominant_topic + 1\n",
    "    df.at[i, 'Dominant_score'] = topic_score_list[dominant_topic]\n",
    "    \n",
    "  return(df)\n",
    "\n",
    "\n",
    "df = store_topic_scores(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
    "df.to_csv(f'twitter_with_LDA.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualize the LDA Topics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution\n",
    "How many words are in each tweet? When working with a large number of tweets, you want to know how big the tweet are as a whole and by topic. Let’s plot the tweet word counts distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lens = [len(d) for d in df.words]\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "plt.figure(figsize=(18,7), dpi=100)\n",
    "sns.distplot(doc_lens)\n",
    "plt.text(.5, .100, \"Mean   : \" + str(round(np.mean(doc_lens), 2)))\n",
    "plt.text(.5, .090, \"Median : \" + str(round(np.median(doc_lens), 2)))\n",
    "plt.text(.5, .080, \"Stdev   : \" + str(round(np.std(doc_lens), 2)))\n",
    "plt.text(.5, .070, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01), 2)))\n",
    "plt.text(.5, .060, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99), 2)))\n",
    "\n",
    "plt.gca().set(ylabel='Number of Tweets', xlabel='Tweet Word Count')\n",
    "plt.tick_params(size=16)\n",
    "plt.xticks(np.linspace(0,27,28))\n",
    "plt.title('Distribution of Tweet Word Counts', fontdict=dict(size=22))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counts by Dominant Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import math\n",
    "cols = [color for name, color in mcolors.XKCD_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS' or 'mcolors.CSS4_COLORS'\n",
    "\n",
    "fig, axes = plt.subplots(math.ceil(num_topics**(1/2)), math.ceil(num_topics**(1/2)), figsize=(16,14), dpi=160, sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):    \n",
    "    df_sub = df.loc[df.Dominant_topic == (i + 1), :]\n",
    "    doc_lens = [len(d) for d in df_sub.words]\n",
    "    ax.hist(doc_lens, color=cols[i])\n",
    "    ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n",
    "    sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx(), bw=1.5)\n",
    "    ax.set(xlim=(0, 28), xlabel='')\n",
    "    ax.set_ylabel('Number of Documents', color=cols[i])\n",
    "    ax.set_title('Topic: '+str(i + 1), fontdict=dict(size=16, color=cols[i]))\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.90)\n",
    "plt.xticks(np.linspace(0,27,28))\n",
    "fig.suptitle('Distribution of Document Word Counts by Dominant Topic', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clouds of Top N Keywords\n",
    "Update the max_words variable below to include more or less words per cloud. The coloring of the topics is used in subsequent vizs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.XKCD_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS', fewer colors: 'mcolors.TABLEAU_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=20,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(math.ceil(num_topics**(1/2)), math.ceil(num_topics**(1/2)), figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    try:\n",
    "      topic_words = dict(topics[i][1])\n",
    "      cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "      plt.gca().imshow(cloud)\n",
    "      plt.gca().set_title('Topic ' + str(i+1), fontdict=dict(size=16))\n",
    "      plt.gca().axis('off')\n",
    "    except:\n",
    "      continue\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Keywords Counts\n",
    "When it comes to the keywords in the topics, the importance (weights) of the keywords matters. Along with that, how frequently the words have appeared in the tweets is also interesting to see.\n",
    "\n",
    "We will plot the word counts and the weights of each keyword in the same chart.\n",
    "\n",
    "Look for words that occur in multiple topics and the ones whose relative frequency is more than the weight. Often such words turn out to be less important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of word counts for each topic\n",
    "from collections import Counter\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in data_ready for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i + 1, weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(math.ceil(num_topics**(1/2)), math.ceil(num_topics**(1/2)), figsize=(20,20), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.XKCD_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i+1, :], color=cols[i+1], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i+1, :], color=cols[i+1], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i+1])\n",
    "    # ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
    "    ax.set_title('Topic: ' + str(i + 1), color=cols[i+1], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i+1, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    try:\n",
    "      ax.legend(loc='upper center'); ax_twin.legend(loc='upper right')\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Chart Colored \n",
    "Each word in a tweet is representative of one of the 4 topics. You can color each word in a given tweet by the topic id it is attributed to.\n",
    "The color of the enclosing rectangle is the topic assigned to the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Coloring of N Tweets\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def sentences_chart(lda_model=lda_model, corpus=corpus, start = 0, end = 13):\n",
    "    corp = corpus[start:end]\n",
    "    mycolors = [color for name, color in mcolors.XKCD_COLORS.items()]\n",
    "\n",
    "    fig, axes = plt.subplots(end-start, 1, figsize=(20, (end-start)*0.95), dpi=160)       \n",
    "    axes[0].axis('off')\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i > 0:\n",
    "            corp_cur = corp[i-1] \n",
    "            topic_percs, wordid_topics, wordid_phivalues = lda_model[corp_cur]\n",
    "            word_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics]    \n",
    "            ax.text(0.01, 0.5, \"Doc \" + str(i) + \": \", verticalalignment='center',\n",
    "                    fontsize=16, color='black', transform=ax.transAxes, fontweight=700)\n",
    "\n",
    "            # Draw Rectange\n",
    "            topic_percs_sorted = sorted(topic_percs, key=lambda x: (x[1]), reverse=True)\n",
    "            ax.add_patch(Rectangle((0.0, 0.05), 0.99, 0.90, fill=None, alpha=1, \n",
    "                                   color=mycolors[topic_percs_sorted[0][0]], linewidth=2))\n",
    "\n",
    "            word_pos = 0.06\n",
    "            for j, (word, topics) in enumerate(word_dominanttopic):\n",
    "                if j < 14:\n",
    "                    ax.text(word_pos, 0.5, word,\n",
    "                            horizontalalignment='left',\n",
    "                            verticalalignment='center',\n",
    "                            fontsize=16, color=mycolors[topics],\n",
    "                            transform=ax.transAxes, fontweight=700)\n",
    "                    word_pos += .009 * len(word)  # to move the word for the next iter\n",
    "                    ax.axis('off')\n",
    "            ax.text(word_pos, 0.5, '. . .',\n",
    "                    horizontalalignment='left',\n",
    "                    verticalalignment='center',\n",
    "                    fontsize=16, color='black',\n",
    "                    transform=ax.transAxes)       \n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.suptitle('Sentence Topic Coloring for Tweets: ' + str(start + 1) + ' to ' + str(end-1), fontsize=22, y=0.95, fontweight=700)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "sentences_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Common Topics\n",
    "What are the most discussed topics in the tweets? We can compute the total number of tweets attributed to each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of the most dominant topics and then the three top keywords in each of those topics\n",
    "def topics_per_document(model, corpus, start=0, end=1):\n",
    "    corpus_sel = corpus[start:end]\n",
    "    dominant_topics = []\n",
    "    topic_percentages = []\n",
    "    for i, corp in enumerate(corpus_sel):\n",
    "        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n",
    "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
    "        dominant_topics.append((i, dominant_topic))\n",
    "        topic_percentages.append(topic_percs)\n",
    "    return(dominant_topics, topic_percentages)\n",
    "\n",
    "dominant_topics, topic_percentages = topics_per_document(model=lda_model, corpus=corpus, end=-1)            \n",
    "\n",
    "# Distribution of Dominant Topics in Each Document\n",
    "df = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\n",
    "dominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\n",
    "df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n",
    "\n",
    "# Total Topic Distribution by actual weight\n",
    "topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n",
    "df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n",
    "\n",
    "# Top 3 Keywords for each Topic\n",
    "topic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n",
    "                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n",
    "\n",
    "df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
    "df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
    "df_top3words.reset_index(level=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two Plots:\n",
    "#    Num tweets per topic by assigning the document to the topic that has the most weight in that document.\n",
    "#    Num tweets per topic by summing up the actual weight contribution of each topic to respective documents.\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 4), dpi=120, sharey=True)\n",
    "\n",
    "# Topic Distribution by Tweet Topics\n",
    "ax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='firebrick')\n",
    "ax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\n",
    "tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x+1)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\n",
    "ax1.xaxis.set_major_formatter(tick_formatter)\n",
    "ax1.set_title('Number of Tweets by Tweet Topic', fontdict=dict(size=10))\n",
    "ax1.set_ylabel('Number of Tweets')\n",
    "# ax1.set_ylim(0, 1000)\n",
    "\n",
    "# Topic Distribution by Topic Weights\n",
    "ax2.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='steelblue')\n",
    "ax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\n",
    "ax2.xaxis.set_major_formatter(tick_formatter)\n",
    "ax2.set_title('Number of Tweets by Topic Weightage', fontdict=dict(size=10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE Clustering Chart\n",
    "Compute the total number of tweets attributed to each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic weights and dominant topics\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "for i, row_list in enumerate(lda_model[corpus]):\n",
    "    topic_weights.append([w for i, w in row_list[0]])\n",
    "\n",
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "\n",
    "# Keep the well separated points (optional)\n",
    "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "# Plot the Topic Clusters using Bokeh\n",
    "output_notebook()\n",
    "n_topics = 4\n",
    "mycolors = np.array([color for name, color in mcolors.XKCD_COLORS.items()])\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
    "              plot_width=900, plot_height=700)\n",
    "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyLDAVis\n",
    "Finally, pyLDAVis is the most commonly used and a nice way to visualise the information contained in a topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some reason, pyLDAVis doesn't seem to work in this AWS kernel. It appears there is a conflict between some of the conda libraries.\n",
    "# import pyLDAvis.gensim\n",
    "# pyLDAvis.enable_notebook()\n",
    "# viz = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "# viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset with LDA Topic Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('twitter_with_LDA.csv')\n",
    "# Convert these numbers to categories\n",
    "df['id'] = df['id'].astype('object')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Images\n",
    "\n",
    "First, we will download all of the images we want to process and store them in S3. Let's first define a method that allows us to download an image from a URL and save it locally. The second block of code simply iterates over our data set and passes the URL and filename to the download_image function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, name):\n",
    "  import requests\n",
    "  file_type = url.split('.')[-1]\n",
    "  img_data = requests.get(url).content\n",
    "  try:\n",
    "    with open(f'images/{name}.{file_type}', 'wb') as handler:\n",
    "      handler.write(img_data)\n",
    "  except:\n",
    "    with open(f'images/{name}.{file_type}', 'wb') as handler:\n",
    "      handler.write(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(df.itertuples()):\n",
    "  if not pd.isnull(row.url):\n",
    "    download_image(row.url, row.id)\n",
    "    print(i, row.url, row.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save them to S3 Bucket\n",
    "Now that we have all of the files stored locally, we need to upload them to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "bucket = \"tmeservy-mldata\"    # replace this with your bucket name\n",
    "prefix = \"photos/twitter\" # replace this with the path to your images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload the files to the S3 bucket\n",
    "images = glob.glob(\"images/*.jpg\")\n",
    "for filename in images:\n",
    "  boto3.Session().resource('s3').Bucket(bucket).upload_file(filename,f'{prefix}/{os.path.basename(filename)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use AWS Rekognition to Scrape Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(802, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweets</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>quotes</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_following</th>\n",
       "      <th>user_listed</th>\n",
       "      <th>user_tweets</th>\n",
       "      <th>...</th>\n",
       "      <th>Dominant_score</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1374863679246659585</td>\n",
       "      <td>2021-03-24 23:20:19+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5992</td>\n",
       "      <td>5887</td>\n",
       "      <td>318</td>\n",
       "      <td>205698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287116</td>\n",
       "      <td>0.012806</td>\n",
       "      <td>0.287116</td>\n",
       "      <td>0.120242</td>\n",
       "      <td>0.012807</td>\n",
       "      <td>0.012805</td>\n",
       "      <td>0.175035</td>\n",
       "      <td>0.012806</td>\n",
       "      <td>0.160291</td>\n",
       "      <td>0.206093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1374863677107671042</td>\n",
       "      <td>2021-03-24 23:20:19+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>854</td>\n",
       "      <td>1</td>\n",
       "      <td>1124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370377</td>\n",
       "      <td>0.370377</td>\n",
       "      <td>0.370356</td>\n",
       "      <td>0.037038</td>\n",
       "      <td>0.037038</td>\n",
       "      <td>0.037038</td>\n",
       "      <td>0.037038</td>\n",
       "      <td>0.037038</td>\n",
       "      <td>0.037038</td>\n",
       "      <td>0.037038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1374863674100183046</td>\n",
       "      <td>2021-03-24 23:20:18+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>4208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111186</td>\n",
       "      <td>0.111115</td>\n",
       "      <td>0.111067</td>\n",
       "      <td>0.111077</td>\n",
       "      <td>0.111132</td>\n",
       "      <td>0.111186</td>\n",
       "      <td>0.111074</td>\n",
       "      <td>0.111106</td>\n",
       "      <td>0.111094</td>\n",
       "      <td>0.111149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1374863668869918721</td>\n",
       "      <td>2021-03-24 23:20:17+00:00</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40783</td>\n",
       "      <td>40417</td>\n",
       "      <td>469</td>\n",
       "      <td>104938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1374863657578946570</td>\n",
       "      <td>2021-03-24 23:20:14+00:00</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>718</td>\n",
       "      <td>1046</td>\n",
       "      <td>14</td>\n",
       "      <td>13761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555388</td>\n",
       "      <td>0.055604</td>\n",
       "      <td>0.055572</td>\n",
       "      <td>0.055577</td>\n",
       "      <td>0.055572</td>\n",
       "      <td>0.555388</td>\n",
       "      <td>0.055572</td>\n",
       "      <td>0.055572</td>\n",
       "      <td>0.055572</td>\n",
       "      <td>0.055572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                 created_at  retweets  likes  replies  \\\n",
       "0  1374863679246659585  2021-03-24 23:20:19+00:00         0      0        0   \n",
       "1  1374863677107671042  2021-03-24 23:20:19+00:00         0      0        0   \n",
       "2  1374863674100183046  2021-03-24 23:20:18+00:00         0      0        0   \n",
       "3  1374863668869918721  2021-03-24 23:20:17+00:00        11      0        0   \n",
       "4  1374863657578946570  2021-03-24 23:20:14+00:00        45      0        0   \n",
       "\n",
       "   quotes  user_followers  user_following  user_listed  user_tweets  ...  \\\n",
       "0       0            5992            5887          318       205698  ...   \n",
       "1       0             137             854            1         1124  ...   \n",
       "2       0              62             164            0         4208  ...   \n",
       "3       0           40783           40417          469       104938  ...   \n",
       "4       0             718            1046           14        13761  ...   \n",
       "\n",
       "   Dominant_score   topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
       "0        0.287116  0.012806  0.287116  0.120242  0.012807  0.012805  0.175035   \n",
       "1        0.370377  0.370377  0.370356  0.037038  0.037038  0.037038  0.037038   \n",
       "2        0.111186  0.111115  0.111067  0.111077  0.111132  0.111186  0.111074   \n",
       "3        0.111111  0.111111  0.111111  0.111111  0.111111  0.111111  0.111111   \n",
       "4        0.555388  0.055604  0.055572  0.055577  0.055572  0.555388  0.055572   \n",
       "\n",
       "    topic_7   topic_8   topic_9  \n",
       "0  0.012806  0.160291  0.206093  \n",
       "1  0.037038  0.037038  0.037038  \n",
       "2  0.111106  0.111094  0.111149  \n",
       "3  0.111111  0.111111  0.111111  \n",
       "4  0.055572  0.055572  0.055572  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('twitter_with_LDA.csv')\n",
    "# Convert these numbers to categories\n",
    "df['id'] = df['id'].astype('object')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, you may want to limit the number of rows that you are processing until you get your code working just right. You can use the following code to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit number of rows for testing-if you don't want to process everything\n",
    "df=df.head(40)\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the rekognition client\n",
    "client = boto3.client('rekognition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the s3 client\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(bucket)\n",
    "#files = my_bucket.objects.filter(Prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous classes when we processed images with Rekognition we retrieved a list of all files in S3 and then passed each file from the list to Rekognition. Here, instead, we will iterate over our dataframe and construct the name of the file we want it to process from the TweetID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TweetID - 1374863668869918721.jpg\n"
     ]
    }
   ],
   "source": [
    "dfFaces = pd.DataFrame([])\n",
    "i=0\n",
    "\n",
    "for row in df.itertuples():\n",
    "    file = f\"{row.id}\"\n",
    "    extension = f\"{row.url.split('.')[-1]}\"\n",
    "    print(f\"Processing TweetID - {file}.{extension}\")\n",
    "    break\n",
    "    if not pd.isnull(file):\n",
    "        #print(i, row[1], file, f\"{prefix}/{row[1]}.{file.split('.')[-1]}\")\n",
    "    \n",
    "    \n",
    "        if extension == 'jpg':\n",
    "            # call rekognition for this next file\n",
    "            response = client.detect_faces(\n",
    "                Image={\n",
    "                    'S3Object': {\n",
    "                        'Bucket': bucket,\n",
    "                        'Name': f\"{prefix}/{file}.{extension}\"\n",
    "                    }\n",
    "                },\n",
    "                Attributes=[\n",
    "                    'ALL',\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # now add all of the facial features for every person found in the photo\n",
    "            for fd in response[\"FaceDetails\"]:    \n",
    "                i=i+1\n",
    "                dfFaces.loc[i,'TweetID'] = file\n",
    "                dfFaces.loc[i,'PersonID'] = i\n",
    "                dfFaces.loc[i,'AgeRange-Low'] = fd[\"AgeRange\"][\"Low\"]\n",
    "                dfFaces.loc[i,'AgeRange-High'] = fd[\"AgeRange\"][\"High\"]\n",
    "                dfFaces.loc[i,'Smile'] = fd[\"Smile\"][\"Value\"]\n",
    "                dfFaces.loc[i,'Gender'] = fd[\"Gender\"][\"Value\"]\n",
    "                dfFaces.loc[i,'Emotion'] = fd[\"Emotions\"][0][\"Type\"] #get dominant emotion\n",
    "                dfFaces.loc[i,'Emotion-Confidence'] = fd[\"Emotions\"][0][\"Confidence\"] #get dominant emotion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetID</th>\n",
       "      <th>PersonID</th>\n",
       "      <th>AgeRange-Low</th>\n",
       "      <th>AgeRange-High</th>\n",
       "      <th>Smile</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Emotion-Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1374863668869918721</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>CALM</td>\n",
       "      <td>45.319225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1374863668869918721</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>FEAR</td>\n",
       "      <td>21.871563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1374863668869918721</td>\n",
       "      <td>3.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>CALM</td>\n",
       "      <td>49.536236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1374863657578946570</td>\n",
       "      <td>4.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Male</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>98.960274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1374863644299821063</td>\n",
       "      <td>5.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>CALM</td>\n",
       "      <td>96.461403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1374863644299821063</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>SAD</td>\n",
       "      <td>91.713036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1374863639908392964</td>\n",
       "      <td>7.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>CALM</td>\n",
       "      <td>45.319225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1374863639908392964</td>\n",
       "      <td>8.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>FEAR</td>\n",
       "      <td>21.871563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1374863639908392964</td>\n",
       "      <td>9.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>CALM</td>\n",
       "      <td>49.536236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1374863636103983106</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>CALM</td>\n",
       "      <td>93.077408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1374863636103983106</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>SAD</td>\n",
       "      <td>99.129227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1374863636103983106</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>SAD</td>\n",
       "      <td>59.317654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1374863636103983106</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>CALM</td>\n",
       "      <td>69.421043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1374863617380655107</td>\n",
       "      <td>14.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>ANGRY</td>\n",
       "      <td>60.187489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1374863617380655107</td>\n",
       "      <td>15.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>CALM</td>\n",
       "      <td>80.783592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1374863617380655107</td>\n",
       "      <td>16.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>SAD</td>\n",
       "      <td>89.231934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1374863617380655107</td>\n",
       "      <td>17.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>41.773830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1374863615229095946</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>CALM</td>\n",
       "      <td>37.100662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1374863615229095946</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>CALM</td>\n",
       "      <td>48.432522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1374863608694251521</td>\n",
       "      <td>20.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>CALM</td>\n",
       "      <td>99.388039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1374863608694251521</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>CALM</td>\n",
       "      <td>90.121529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1374863608694251521</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>CALM</td>\n",
       "      <td>52.160126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1374863608652386308</td>\n",
       "      <td>23.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>FEAR</td>\n",
       "      <td>49.401100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1374863608652386308</td>\n",
       "      <td>24.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>CALM</td>\n",
       "      <td>94.733582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1374863571721580547</td>\n",
       "      <td>25.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>CALM</td>\n",
       "      <td>93.077408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1374863571721580547</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>SAD</td>\n",
       "      <td>99.129227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1374863571721580547</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>SAD</td>\n",
       "      <td>59.317654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1374863571721580547</td>\n",
       "      <td>28.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>CALM</td>\n",
       "      <td>69.421043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1374863538255237121</td>\n",
       "      <td>29.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>ANGRY</td>\n",
       "      <td>82.984207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1374863514616139789</td>\n",
       "      <td>30.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>CALM</td>\n",
       "      <td>63.218628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1374863512175009797</td>\n",
       "      <td>31.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>CALM</td>\n",
       "      <td>93.077408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1374863512175009797</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>SAD</td>\n",
       "      <td>99.129227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1374863512175009797</td>\n",
       "      <td>33.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>SAD</td>\n",
       "      <td>59.317654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1374863512175009797</td>\n",
       "      <td>34.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>CALM</td>\n",
       "      <td>69.421043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1374863465706352640</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>CALM</td>\n",
       "      <td>88.989517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1374863465706352640</td>\n",
       "      <td>36.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>CALM</td>\n",
       "      <td>86.907684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1374863442251718662</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Female</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>85.534073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1374863427831803906</td>\n",
       "      <td>38.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>CALM</td>\n",
       "      <td>96.102859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1374863407371927552</td>\n",
       "      <td>39.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>CALM</td>\n",
       "      <td>67.205200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1374863368461246467</td>\n",
       "      <td>40.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Female</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>95.928314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1374863360806690816</td>\n",
       "      <td>41.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>CALM</td>\n",
       "      <td>91.506256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1374863352166486016</td>\n",
       "      <td>42.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>CALM</td>\n",
       "      <td>67.557930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1374863352166486016</td>\n",
       "      <td>43.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>ANGRY</td>\n",
       "      <td>85.925522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1374863352166486016</td>\n",
       "      <td>44.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>ANGRY</td>\n",
       "      <td>49.274315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1374863352166486016</td>\n",
       "      <td>45.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>True</td>\n",
       "      <td>Female</td>\n",
       "      <td>HAPPY</td>\n",
       "      <td>48.935390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1374863352166486016</td>\n",
       "      <td>46.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>CALM</td>\n",
       "      <td>53.075512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1374863352166486016</td>\n",
       "      <td>47.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>CALM</td>\n",
       "      <td>25.815531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1374863352166486016</td>\n",
       "      <td>48.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Male</td>\n",
       "      <td>CALM</td>\n",
       "      <td>42.819813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1374863352166486016</td>\n",
       "      <td>49.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>SAD</td>\n",
       "      <td>92.020439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1374863352166486016</td>\n",
       "      <td>50.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Female</td>\n",
       "      <td>CALM</td>\n",
       "      <td>46.342144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                TweetID  PersonID  AgeRange-Low  AgeRange-High  Smile  Gender  \\\n",
       "1   1374863668869918721       1.0          21.0           33.0  False    Male   \n",
       "2   1374863668869918721       2.0          36.0           54.0  False    Male   \n",
       "3   1374863668869918721       3.0          50.0           68.0  False  Female   \n",
       "4   1374863657578946570       4.0          51.0           69.0   True    Male   \n",
       "5   1374863644299821063       5.0          22.0           34.0  False    Male   \n",
       "6   1374863644299821063       6.0          13.0           25.0  False  Female   \n",
       "7   1374863639908392964       7.0          21.0           33.0  False    Male   \n",
       "8   1374863639908392964       8.0          36.0           54.0  False    Male   \n",
       "9   1374863639908392964       9.0          50.0           68.0  False  Female   \n",
       "10  1374863636103983106      10.0          27.0           43.0  False  Female   \n",
       "11  1374863636103983106      11.0           3.0            9.0  False  Female   \n",
       "12  1374863636103983106      12.0           3.0           11.0  False    Male   \n",
       "13  1374863636103983106      13.0           9.0           19.0  False    Male   \n",
       "14  1374863617380655107      14.0          39.0           57.0  False    Male   \n",
       "15  1374863617380655107      15.0          41.0           59.0  False  Female   \n",
       "16  1374863617380655107      16.0          48.0           66.0  False  Female   \n",
       "17  1374863617380655107      17.0          36.0           54.0  False    Male   \n",
       "18  1374863615229095946      18.0          15.0           27.0  False  Female   \n",
       "19  1374863615229095946      19.0           0.0            3.0  False    Male   \n",
       "20  1374863608694251521      20.0          29.0           45.0  False    Male   \n",
       "21  1374863608694251521      21.0           6.0           16.0  False    Male   \n",
       "22  1374863608694251521      22.0          22.0           34.0  False  Female   \n",
       "23  1374863608652386308      23.0           9.0           19.0  False  Female   \n",
       "24  1374863608652386308      24.0          19.0           31.0  False  Female   \n",
       "25  1374863571721580547      25.0          27.0           43.0  False  Female   \n",
       "26  1374863571721580547      26.0           3.0            9.0  False  Female   \n",
       "27  1374863571721580547      27.0           3.0           11.0  False    Male   \n",
       "28  1374863571721580547      28.0           9.0           19.0  False    Male   \n",
       "29  1374863538255237121      29.0          25.0           39.0  False    Male   \n",
       "30  1374863514616139789      30.0          15.0           27.0  False    Male   \n",
       "31  1374863512175009797      31.0          27.0           43.0  False  Female   \n",
       "32  1374863512175009797      32.0           3.0            9.0  False  Female   \n",
       "33  1374863512175009797      33.0           3.0           11.0  False    Male   \n",
       "34  1374863512175009797      34.0           9.0           19.0  False    Male   \n",
       "35  1374863465706352640      35.0          14.0           26.0  False    Male   \n",
       "36  1374863465706352640      36.0          13.0           23.0  False    Male   \n",
       "37  1374863442251718662      37.0           3.0            9.0   True  Female   \n",
       "38  1374863427831803906      38.0          24.0           38.0  False    Male   \n",
       "39  1374863407371927552      39.0          23.0           35.0  False    Male   \n",
       "40  1374863368461246467      40.0          22.0           34.0   True  Female   \n",
       "41  1374863360806690816      41.0          40.0           58.0  False  Female   \n",
       "42  1374863352166486016      42.0          36.0           52.0  False  Female   \n",
       "43  1374863352166486016      43.0          33.0           49.0  False    Male   \n",
       "44  1374863352166486016      44.0          35.0           51.0  False    Male   \n",
       "45  1374863352166486016      45.0           9.0           19.0   True  Female   \n",
       "46  1374863352166486016      46.0          37.0           55.0  False  Female   \n",
       "47  1374863352166486016      47.0          17.0           29.0  False    Male   \n",
       "48  1374863352166486016      48.0          14.0           26.0  False    Male   \n",
       "49  1374863352166486016      49.0           4.0           14.0  False  Female   \n",
       "50  1374863352166486016      50.0          38.0           56.0  False  Female   \n",
       "\n",
       "   Emotion  Emotion-Confidence  \n",
       "1     CALM           45.319225  \n",
       "2     FEAR           21.871563  \n",
       "3     CALM           49.536236  \n",
       "4    HAPPY           98.960274  \n",
       "5     CALM           96.461403  \n",
       "6      SAD           91.713036  \n",
       "7     CALM           45.319225  \n",
       "8     FEAR           21.871563  \n",
       "9     CALM           49.536236  \n",
       "10    CALM           93.077408  \n",
       "11     SAD           99.129227  \n",
       "12     SAD           59.317654  \n",
       "13    CALM           69.421043  \n",
       "14   ANGRY           60.187489  \n",
       "15    CALM           80.783592  \n",
       "16     SAD           89.231934  \n",
       "17   HAPPY           41.773830  \n",
       "18    CALM           37.100662  \n",
       "19    CALM           48.432522  \n",
       "20    CALM           99.388039  \n",
       "21    CALM           90.121529  \n",
       "22    CALM           52.160126  \n",
       "23    FEAR           49.401100  \n",
       "24    CALM           94.733582  \n",
       "25    CALM           93.077408  \n",
       "26     SAD           99.129227  \n",
       "27     SAD           59.317654  \n",
       "28    CALM           69.421043  \n",
       "29   ANGRY           82.984207  \n",
       "30    CALM           63.218628  \n",
       "31    CALM           93.077408  \n",
       "32     SAD           99.129227  \n",
       "33     SAD           59.317654  \n",
       "34    CALM           69.421043  \n",
       "35    CALM           88.989517  \n",
       "36    CALM           86.907684  \n",
       "37   HAPPY           85.534073  \n",
       "38    CALM           96.102859  \n",
       "39    CALM           67.205200  \n",
       "40   HAPPY           95.928314  \n",
       "41    CALM           91.506256  \n",
       "42    CALM           67.557930  \n",
       "43   ANGRY           85.925522  \n",
       "44   ANGRY           49.274315  \n",
       "45   HAPPY           48.935390  \n",
       "46    CALM           53.075512  \n",
       "47    CALM           25.815531  \n",
       "48    CALM           42.819813  \n",
       "49     SAD           92.020439  \n",
       "50    CALM           46.342144  "
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfFaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we extracted information from people that were detected in the image. Of course, we could have used other Rekgonition methods to extract text or identify different types of objects that were identified. We now need to summarize this data into a single row for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count-People</th>\n",
       "      <th>Avg-AgeRange-Low</th>\n",
       "      <th>Avg-AgeRange-High</th>\n",
       "      <th>Count-Smile</th>\n",
       "      <th>Percent-Smile</th>\n",
       "      <th>ANGRY</th>\n",
       "      <th>CALM</th>\n",
       "      <th>FEAR</th>\n",
       "      <th>HAPPY</th>\n",
       "      <th>SAD</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TweetID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1374863352166486016</th>\n",
       "      <td>9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374863360806690816</th>\n",
       "      <td>1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374863368461246467</th>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374863407371927552</th>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374863427831803906</th>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374863442251718662</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374863465706352640</th>\n",
       "      <td>2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374863512175009797</th>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374863514616139789</th>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374863538255237121</th>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374863571721580547</th>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374863608652386308</th>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374863608694251521</th>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374863615229095946</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374863617380655107</th>\n",
       "      <td>4</td>\n",
       "      <td>36.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374863636103983106</th>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374863639908392964</th>\n",
       "      <td>3</td>\n",
       "      <td>21.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374863644299821063</th>\n",
       "      <td>2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374863657578946570</th>\n",
       "      <td>1</td>\n",
       "      <td>51.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374863668869918721</th>\n",
       "      <td>3</td>\n",
       "      <td>21.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Count-People  Avg-AgeRange-Low  Avg-AgeRange-High  \\\n",
       "TweetID                                                                  \n",
       "1374863352166486016             9               4.0               56.0   \n",
       "1374863360806690816             1              40.0               58.0   \n",
       "1374863368461246467             1              22.0               34.0   \n",
       "1374863407371927552             1              23.0               35.0   \n",
       "1374863427831803906             1              24.0               38.0   \n",
       "1374863442251718662             1               3.0                9.0   \n",
       "1374863465706352640             2              13.0               26.0   \n",
       "1374863512175009797             4               3.0               43.0   \n",
       "1374863514616139789             1              15.0               27.0   \n",
       "1374863538255237121             1              25.0               39.0   \n",
       "1374863571721580547             4               3.0               43.0   \n",
       "1374863608652386308             2               9.0               31.0   \n",
       "1374863608694251521             3               6.0               45.0   \n",
       "1374863615229095946             2               0.0               27.0   \n",
       "1374863617380655107             4              36.0               66.0   \n",
       "1374863636103983106             4               3.0               43.0   \n",
       "1374863639908392964             3              21.0               68.0   \n",
       "1374863644299821063             2              13.0               34.0   \n",
       "1374863657578946570             1              51.0               69.0   \n",
       "1374863668869918721             3              21.0               68.0   \n",
       "\n",
       "                     Count-Smile  Percent-Smile  ANGRY  CALM  FEAR  HAPPY  \\\n",
       "TweetID                                                                     \n",
       "1374863352166486016          1.0            0.0    2.0   5.0   0.0    1.0   \n",
       "1374863360806690816          0.0            0.0    0.0   1.0   0.0    0.0   \n",
       "1374863368461246467          1.0            0.0    0.0   0.0   0.0    1.0   \n",
       "1374863407371927552          0.0            0.0    0.0   1.0   0.0    0.0   \n",
       "1374863427831803906          0.0            0.0    0.0   1.0   0.0    0.0   \n",
       "1374863442251718662          1.0            0.0    0.0   0.0   0.0    1.0   \n",
       "1374863465706352640          0.0            0.0    0.0   2.0   0.0    0.0   \n",
       "1374863512175009797          0.0            0.0    0.0   2.0   0.0    0.0   \n",
       "1374863514616139789          0.0            0.0    0.0   1.0   0.0    0.0   \n",
       "1374863538255237121          0.0            0.0    1.0   0.0   0.0    0.0   \n",
       "1374863571721580547          0.0            0.0    0.0   2.0   0.0    0.0   \n",
       "1374863608652386308          0.0            0.0    0.0   1.0   1.0    0.0   \n",
       "1374863608694251521          0.0            0.0    0.0   3.0   0.0    0.0   \n",
       "1374863615229095946          0.0            0.0    0.0   2.0   0.0    0.0   \n",
       "1374863617380655107          0.0            0.0    1.0   1.0   0.0    1.0   \n",
       "1374863636103983106          0.0            0.0    0.0   2.0   0.0    0.0   \n",
       "1374863639908392964          0.0            0.0    0.0   2.0   1.0    0.0   \n",
       "1374863644299821063          0.0            0.0    0.0   1.0   0.0    0.0   \n",
       "1374863657578946570          1.0            0.0    0.0   0.0   0.0    1.0   \n",
       "1374863668869918721          0.0            0.0    0.0   2.0   1.0    0.0   \n",
       "\n",
       "                     SAD  Female  Male  \n",
       "TweetID                                 \n",
       "1374863352166486016  1.0     5.0   4.0  \n",
       "1374863360806690816  0.0     1.0   0.0  \n",
       "1374863368461246467  0.0     1.0   0.0  \n",
       "1374863407371927552  0.0     0.0   1.0  \n",
       "1374863427831803906  0.0     0.0   1.0  \n",
       "1374863442251718662  0.0     1.0   0.0  \n",
       "1374863465706352640  0.0     0.0   2.0  \n",
       "1374863512175009797  2.0     2.0   2.0  \n",
       "1374863514616139789  0.0     0.0   1.0  \n",
       "1374863538255237121  0.0     0.0   1.0  \n",
       "1374863571721580547  2.0     2.0   2.0  \n",
       "1374863608652386308  0.0     2.0   0.0  \n",
       "1374863608694251521  0.0     1.0   2.0  \n",
       "1374863615229095946  0.0     1.0   1.0  \n",
       "1374863617380655107  1.0     2.0   2.0  \n",
       "1374863636103983106  2.0     2.0   2.0  \n",
       "1374863639908392964  0.0     1.0   2.0  \n",
       "1374863644299821063  1.0     1.0   1.0  \n",
       "1374863657578946570  0.0     0.0   1.0  \n",
       "1374863668869918721  0.0     1.0   2.0  "
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfFaceSummary = pd.DataFrame([])\n",
    "\n",
    "#summarize stats per file\n",
    "#aggregate functions include min, max, mean, count, and more.\n",
    "dfFaceSummary['Count-People'] = dfFaces.groupby('TweetID')['PersonID'].count()\n",
    "dfFaceSummary['Avg-AgeRange-Low'] = dfFaces.groupby('TweetID')['AgeRange-Low'].min()\n",
    "dfFaceSummary['Avg-AgeRange-High'] = dfFaces.groupby('TweetID')['AgeRange-High'].max()\n",
    "dfFaceSummary['Count-Smile'] = dfFaces[dfFaces['Smile']==True].groupby('TweetID')['Smile'].count()\n",
    "dfFaceSummary['Count-Smile'] = dfFaceSummary['Count-Smile'].fillna(0)\n",
    "dfFaceSummary['Percent-Smile'] = dfFaceSummary['Count-Smile']/df2['Count-People']\n",
    "dfFaceSummary['Percent-Smile'] = dfFaceSummary['Percent-Smile'].fillna(0)\n",
    "\n",
    "dfFaceSummary = dfFaceSummary.merge(dfFaces.groupby('TweetID')['Emotion'].value_counts().unstack().fillna(0), on='TweetID')\n",
    "dfFaceSummary = dfFaceSummary.merge(dfFaces.groupby('TweetID')['Gender'].value_counts().unstack().fillna(0), on='TweetID')\n",
    "\n",
    "#print out \n",
    "dfFaceSummary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have summary information, we need to merge it back to our data frame and save it to a CSV file so that we can use it during modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweets</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>quotes</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_following</th>\n",
       "      <th>user_listed</th>\n",
       "      <th>user_tweets</th>\n",
       "      <th>...</th>\n",
       "      <th>Avg-AgeRange-High</th>\n",
       "      <th>Count-Smile</th>\n",
       "      <th>Percent-Smile</th>\n",
       "      <th>ANGRY</th>\n",
       "      <th>CALM</th>\n",
       "      <th>FEAR</th>\n",
       "      <th>HAPPY</th>\n",
       "      <th>SAD</th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1374863668869918721</td>\n",
       "      <td>2021-03-24 23:20:17+00:00</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40783</td>\n",
       "      <td>40417</td>\n",
       "      <td>469</td>\n",
       "      <td>104938</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1374863657578946570</td>\n",
       "      <td>2021-03-24 23:20:14+00:00</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>718</td>\n",
       "      <td>1046</td>\n",
       "      <td>14</td>\n",
       "      <td>13761</td>\n",
       "      <td>...</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1374863644299821063</td>\n",
       "      <td>2021-03-24 23:20:11+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12328</td>\n",
       "      <td>13559</td>\n",
       "      <td>51</td>\n",
       "      <td>310403</td>\n",
       "      <td>...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1374863639908392964</td>\n",
       "      <td>2021-03-24 23:20:10+00:00</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22252</td>\n",
       "      <td>8135</td>\n",
       "      <td>221</td>\n",
       "      <td>194717</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1374863636103983106</td>\n",
       "      <td>2021-03-24 23:20:09+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>818</td>\n",
       "      <td>1006</td>\n",
       "      <td>6</td>\n",
       "      <td>59686</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1374863617380655107</td>\n",
       "      <td>2021-03-24 23:20:04+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>759</td>\n",
       "      <td>0</td>\n",
       "      <td>15238</td>\n",
       "      <td>...</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1374863615229095946</td>\n",
       "      <td>2021-03-24 23:20:04+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2106</td>\n",
       "      <td>1287</td>\n",
       "      <td>40</td>\n",
       "      <td>5584</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1374863608694251521</td>\n",
       "      <td>2021-03-24 23:20:02+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3060</td>\n",
       "      <td>3036</td>\n",
       "      <td>18</td>\n",
       "      <td>82103</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1374863608652386308</td>\n",
       "      <td>2021-03-24 23:20:02+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>545</td>\n",
       "      <td>680</td>\n",
       "      <td>3</td>\n",
       "      <td>3475</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1374863571721580547</td>\n",
       "      <td>2021-03-24 23:19:53+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>373</td>\n",
       "      <td>4</td>\n",
       "      <td>10349</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1374863538255237121</td>\n",
       "      <td>2021-03-24 23:19:45+00:00</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1280</td>\n",
       "      <td>149</td>\n",
       "      <td>5</td>\n",
       "      <td>65923</td>\n",
       "      <td>...</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1374863514616139789</td>\n",
       "      <td>2021-03-24 23:19:40+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>521</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1374863512175009797</td>\n",
       "      <td>2021-03-24 23:19:39+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>439</td>\n",
       "      <td>3349</td>\n",
       "      <td>2</td>\n",
       "      <td>23421</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1374863465706352640</td>\n",
       "      <td>2021-03-24 23:19:28+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>229</td>\n",
       "      <td>296</td>\n",
       "      <td>13</td>\n",
       "      <td>325</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1374863442251718662</td>\n",
       "      <td>2021-03-24 23:19:23+00:00</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>806</td>\n",
       "      <td>641</td>\n",
       "      <td>2</td>\n",
       "      <td>56861</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1374863427831803906</td>\n",
       "      <td>2021-03-24 23:19:19+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1077</td>\n",
       "      <td>4948</td>\n",
       "      <td>5</td>\n",
       "      <td>10631</td>\n",
       "      <td>...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1374863407371927552</td>\n",
       "      <td>2021-03-24 23:19:14+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>456</td>\n",
       "      <td>4</td>\n",
       "      <td>513</td>\n",
       "      <td>...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1374863368461246467</td>\n",
       "      <td>2021-03-24 23:19:05+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27244</td>\n",
       "      <td>25323</td>\n",
       "      <td>157</td>\n",
       "      <td>662617</td>\n",
       "      <td>...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1374863360806690816</td>\n",
       "      <td>2021-03-24 23:19:03+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3606</td>\n",
       "      <td>4405</td>\n",
       "      <td>102</td>\n",
       "      <td>22808</td>\n",
       "      <td>...</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1374863352166486016</td>\n",
       "      <td>2021-03-24 23:19:01+00:00</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>242</td>\n",
       "      <td>383</td>\n",
       "      <td>2</td>\n",
       "      <td>19967</td>\n",
       "      <td>...</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                 created_at  retweets  likes  replies  \\\n",
       "0   1374863668869918721  2021-03-24 23:20:17+00:00        11      0        0   \n",
       "1   1374863657578946570  2021-03-24 23:20:14+00:00        45      0        0   \n",
       "2   1374863644299821063  2021-03-24 23:20:11+00:00         2      0        0   \n",
       "3   1374863639908392964  2021-03-24 23:20:10+00:00        11      0        0   \n",
       "4   1374863636103983106  2021-03-24 23:20:09+00:00         5      0        0   \n",
       "5   1374863617380655107  2021-03-24 23:20:04+00:00         0      0        0   \n",
       "6   1374863615229095946  2021-03-24 23:20:04+00:00         0      0        0   \n",
       "7   1374863608694251521  2021-03-24 23:20:02+00:00         0      0        0   \n",
       "8   1374863608652386308  2021-03-24 23:20:02+00:00         0      0        0   \n",
       "9   1374863571721580547  2021-03-24 23:19:53+00:00         5      0        0   \n",
       "10  1374863538255237121  2021-03-24 23:19:45+00:00        38      0        0   \n",
       "11  1374863514616139789  2021-03-24 23:19:40+00:00         0      0        0   \n",
       "12  1374863512175009797  2021-03-24 23:19:39+00:00         5      0        0   \n",
       "13  1374863465706352640  2021-03-24 23:19:28+00:00         0      0        0   \n",
       "14  1374863442251718662  2021-03-24 23:19:23+00:00       117      0        0   \n",
       "15  1374863427831803906  2021-03-24 23:19:19+00:00         0      0        0   \n",
       "16  1374863407371927552  2021-03-24 23:19:14+00:00         0      0        0   \n",
       "17  1374863368461246467  2021-03-24 23:19:05+00:00         0      0        0   \n",
       "18  1374863360806690816  2021-03-24 23:19:03+00:00         3      0        0   \n",
       "19  1374863352166486016  2021-03-24 23:19:01+00:00        30      0        0   \n",
       "\n",
       "    quotes  user_followers  user_following  user_listed  user_tweets  ...  \\\n",
       "0        0           40783           40417          469       104938  ...   \n",
       "1        0             718            1046           14        13761  ...   \n",
       "2        0           12328           13559           51       310403  ...   \n",
       "3        0           22252            8135          221       194717  ...   \n",
       "4        0             818            1006            6        59686  ...   \n",
       "5        0              74             759            0        15238  ...   \n",
       "6        0            2106            1287           40         5584  ...   \n",
       "7        0            3060            3036           18        82103  ...   \n",
       "8        0             545             680            3         3475  ...   \n",
       "9        0             155             373            4        10349  ...   \n",
       "10       0            1280             149            5        65923  ...   \n",
       "11       0              22             117            0          521  ...   \n",
       "12       0             439            3349            2        23421  ...   \n",
       "13       0             229             296           13          325  ...   \n",
       "14       0             806             641            2        56861  ...   \n",
       "15       0            1077            4948            5        10631  ...   \n",
       "16       0             165             456            4          513  ...   \n",
       "17       0           27244           25323          157       662617  ...   \n",
       "18       0            3606            4405          102        22808  ...   \n",
       "19       0             242             383            2        19967  ...   \n",
       "\n",
       "    Avg-AgeRange-High Count-Smile Percent-Smile ANGRY CALM  FEAR  HAPPY  SAD  \\\n",
       "0                68.0         0.0           0.0   0.0  2.0   1.0    0.0  0.0   \n",
       "1                69.0         1.0           0.0   0.0  0.0   0.0    1.0  0.0   \n",
       "2                34.0         0.0           0.0   0.0  1.0   0.0    0.0  1.0   \n",
       "3                68.0         0.0           0.0   0.0  2.0   1.0    0.0  0.0   \n",
       "4                43.0         0.0           0.0   0.0  2.0   0.0    0.0  2.0   \n",
       "5                66.0         0.0           0.0   1.0  1.0   0.0    1.0  1.0   \n",
       "6                27.0         0.0           0.0   0.0  2.0   0.0    0.0  0.0   \n",
       "7                45.0         0.0           0.0   0.0  3.0   0.0    0.0  0.0   \n",
       "8                31.0         0.0           0.0   0.0  1.0   1.0    0.0  0.0   \n",
       "9                43.0         0.0           0.0   0.0  2.0   0.0    0.0  2.0   \n",
       "10               39.0         0.0           0.0   1.0  0.0   0.0    0.0  0.0   \n",
       "11               27.0         0.0           0.0   0.0  1.0   0.0    0.0  0.0   \n",
       "12               43.0         0.0           0.0   0.0  2.0   0.0    0.0  2.0   \n",
       "13               26.0         0.0           0.0   0.0  2.0   0.0    0.0  0.0   \n",
       "14                9.0         1.0           0.0   0.0  0.0   0.0    1.0  0.0   \n",
       "15               38.0         0.0           0.0   0.0  1.0   0.0    0.0  0.0   \n",
       "16               35.0         0.0           0.0   0.0  1.0   0.0    0.0  0.0   \n",
       "17               34.0         1.0           0.0   0.0  0.0   0.0    1.0  0.0   \n",
       "18               58.0         0.0           0.0   0.0  1.0   0.0    0.0  0.0   \n",
       "19               56.0         1.0           0.0   2.0  5.0   0.0    1.0  1.0   \n",
       "\n",
       "    Female  Male  \n",
       "0      1.0   2.0  \n",
       "1      0.0   1.0  \n",
       "2      1.0   1.0  \n",
       "3      1.0   2.0  \n",
       "4      2.0   2.0  \n",
       "5      2.0   2.0  \n",
       "6      1.0   1.0  \n",
       "7      1.0   2.0  \n",
       "8      2.0   0.0  \n",
       "9      2.0   2.0  \n",
       "10     0.0   1.0  \n",
       "11     0.0   1.0  \n",
       "12     2.0   2.0  \n",
       "13     0.0   2.0  \n",
       "14     1.0   0.0  \n",
       "15     0.0   1.0  \n",
       "16     0.0   1.0  \n",
       "17     1.0   0.0  \n",
       "18     1.0   0.0  \n",
       "19     5.0   4.0  \n",
       "\n",
       "[20 rows x 41 columns]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the tweet id is an object, so we need to convert it to the same data type as our id\n",
    "dfFaceSummary.index = dfFaceSummary.index.astype(int)\n",
    "\n",
    "#Now merge the frames together\n",
    "df = df.merge(dfFaceSummary, left_on='id', right_on='TweetID')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now save this out to a new csv\n",
    "df.to_csv('twitter_with_LDA_and_image_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor # Import Decision Tree Regression algorithm\n",
    "from sklearn.ensemble import GradientBoostingRegressor # Import XGBoost algorithm \n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "# for a completelist of available algorithms: https://scikit-learn.org/stable/supervised_learning.html\n",
    "# Which one should I use?: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "\n",
    "df = pd.read_csv('twitter_with_LDA_and_image_data.csv')\n",
    "# Convert these numbers to categories\n",
    "df['id'] = df['id'].astype('object')\n",
    "print(df.shape)\n",
    "\n",
    "# Determine what you want to predict:\n",
    "label = 'retweets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the columns so that we can identify which ones we want to drop (e.g. unique identifiers, original text before processing, image file names, dates)\n",
    "for col in df.columns:\n",
    "  print(f'\\'{col}\\', ', end=\"\")\n",
    "\n",
    "drop_list = ['id', 'created_at', 'url', 'text', 'words', 'File', 'PersonID', 'topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5', 'topic_6', 'topic_7', 'topic_8', 'topic_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate anything remaining in the drop list without throwing an error in case we removed it earlier\n",
    "for col in df.columns:\n",
    "  if col in drop_list:\n",
    "    try:\n",
    "      df.drop(columns=[col], inplace=True)\n",
    "    except:\n",
    "      continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)       # Remove any rows with null values\n",
    "print(f'Records: {len(df)}')  # Count and print the number of rows remaining\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Dominant_topic'] = df['Dominant_topic'].astype('object') # Topics are categorical so this needs to be modified since it's a number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy codes for all features and not the label\n",
    "for col in df.columns:\n",
    "  if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "    df = pd.get_dummies(df, columns=[col], prefix=col)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate columns with only one unique value:\n",
    "for col in df.columns:\n",
    "  if (df[col].nunique() < 2):\n",
    "    try:\n",
    "      df.drop(columns=[col], inplace=True)\n",
    "    except:\n",
    "      continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset in features and target variable\n",
    "\n",
    "y = df[label] # Label\n",
    "X = df.drop(columns=[label]) # Features\n",
    "X = X.select_dtypes(np.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Linear Regression is only for model interpretation purposes\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Run the multiple linear regression model\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# View results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree regressor object\n",
    "clf = DecisionTreeRegressor()\n",
    "\n",
    "# Train Decision Tree regressor\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels for test dataset\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,})\n",
    "output_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scikit-learn metrics module. See complete list of Classification metrics here: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "from sklearn import metrics\n",
    "    \n",
    "print(f'R squared:\\t{metrics.r2_score(y_test, y_pred)}')\n",
    "print(f'MAE:\\t\\t{metrics.mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'RMSE:\\t\\t{metrics.mean_squared_error(y_test, y_pred)**(1/2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XGBoost regressor object\n",
    "clr = GradientBoostingRegressor()\n",
    "\n",
    "# Train Decision Tree regression\n",
    "clr = clf.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels for test dataset\n",
    "y_pred = clr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred,})\n",
    "output_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "    \n",
    "print(f'R squared:\\t{metrics.r2_score(y_test, y_pred)}')\n",
    "print(f'MAE:\\t\\t{metrics.mean_absolute_error(y_test, y_pred)}')\n",
    "print(f'RMSE:\\t\\t{metrics.mean_squared_error(y_test, y_pred)**(1/2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model with the highest fit metric\n",
    "pickle.dump(clr, open('stored_model.sav', 'wb'))  # OPTION 1: pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...some time later\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# OPTION 1: Using pickle\n",
    "# load the model from 'stored_model.sav'\n",
    "loaded_model = pickle.load(open('stored_model.sav', 'rb'))\n",
    "\n",
    "# for a single prediction, enter a row of data and reshape into numpy array\n",
    "case = X_test.iloc[0]\n",
    "print(f'Single prediction:\\t{loaded_model.predict(np.array(case).reshape(1, -1))[0]}\\n\\n{case}\\n')\n",
    "\n",
    "# for a batch prediction, enter a Pandas DataFrame or a Numpy array of arrays\n",
    "predictions = loaded_model.predict(X_test) \n",
    "batch_results = pd.DataFrame({'Actual':y_test, 'Predicted':predictions, 'Diff':(predictions - y_test)})\n",
    "print(f'MAE:\\t{batch_results.Diff.abs().mean()}\\n')\n",
    "batch_results.head(5)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
